---
title: "손실 함수 (MSE, 교차 엔트로피, Focal Loss)"
date: "2025-07-03"
tags: ["딥러닝", "신경망", "손실 함수", "MSE", "교차 엔트로피", "Focal Loss"]
difficulty: "medium"
---

# 손실 함수 (MSE, 교차 엔트로피, Focal Loss)

## 1. 핵심 개념 (Core Concept)

손실 함수(Loss Function)는 신경망 모델의 예측값과 실제 정답값 사이의 차이, 즉 **오차(Error)**를 측정하는 함수임. 딥러닝 모델은 이 손실 함수가 반환하는 값을 최소화하는 방향으로 가중치를 업데이트하며 학습을 진행함. 따라서 해결하고자 하는 문제(회귀, 분류 등)에 적합한 손실 함수를 선택하는 것이 매우 중요함.

---

## 2. 상세 설명 (Detailed Explanation)

### 2.1 평균 제곱 오차 (Mean Squared Error, MSE)

MSE는 **회귀(Regression)** 문제에서 가장 널리 사용되는 손실 함수임. 예측값과 실제값 차이의 제곱을 평균하여 계산하며, 오차가 클수록 더 큰 패널티를 부여하는 특징이 있음.

*   **수식**: `MSE = (1/n) * Σ(y_true - y_pred)^2`
*   **특징**:
    *   **장점**: 오차의 제곱을 사용하므로 예측값과 실제값의 차이가 클 때 더 민감하게 반응하며, 수학적으로 다루기 쉽고 안정적인 해를 제공함.
    *   **단점**: 이상치(outlier)에 매우 민감하게 반응하여 모델 학습을 불안정하게 만들 수 있음. 예를 들어, 하나의 큰 오차가 전체 손실 값에 지배적인 영향을 줄 수 있음.

### 2.2 교차 엔트로피 오차 (Cross-Entropy Error, CEE)

교차 엔트로피는 **분류(Classification)** 문제, 특히 다중 클래스 분류에서 표준적으로 사용되는 손실 함수임. 모델이 예측한 확률 분포와 실제 정답의 확률 분포 사이의 차이를 측정함.

*   **수식 (Binary Cross-Entropy)**: `BCE = - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))`
*   **수식 (Categorical Cross-Entropy)**: `CCE = - Σ(y_true * log(y_pred))`
*   **특징**:
    *   **장점**: 모델이 틀린 예측을 했을 때 (예: 실제 정답이 1인데 예측 확률이 0에 가까울 때) 손실 값이 무한대에 가깝게 치솟아 매우 큰 패널티를 부여함. 이는 모델이 빠르게 오차를 수정하도록 유도함.
    *   **단점**: 클래스 불균형(class imbalance) 문제에 취약할 수 있음. 즉, 데이터 수가 많은 클래스에 대한 손실이 전체 손실을 주도하여 소수 클래스에 대한 학습이 제대로 이루어지지 않을 수 있음.

### 2.3 Focal Loss

Focal Loss는 교차 엔트로피를 개선하여 **클래스 불균형** 문제를 해결하기 위해 제안된 손실 함수임. 특히 객체 탐지(Object Detection)와 같이 배경(background) 클래스가 객체(foreground) 클래스보다 압도적으로 많은 경우에 효과적임.

*   **수식**: `Focal Loss = -α * (1 - p_t)^γ * log(p_t)`
    *   `p_t`: 정답 클래스에 대한 모델의 예측 확률
    *   `γ (gamma)`: Focusing 파라미터. `γ > 0`
    *   `α (alpha)`: 클래스 가중치 파라미터
*   **특징**:
    *   **핵심 아이디어**: 예측하기 쉬운 샘플(well-classified examples, `p_t`가 높은 경우)의 손실은 줄이고, 예측하기 어려운 샘플(hard examples, `p_t`가 낮은 경우)의 손실에 더 집중(focus)하도록 함.
    *   `γ` 값이 커질수록, 모델이 이미 잘 맞추고 있는 샘플에 대한 손실 기여도를 크게 줄여, 학습이 어려운 소수 클래스나 경계선상의 샘플에 더 집중하게 만듦.

---

## 3. 예시 (Example)

### 사용 사례 비교 (Use Case)

| 손실 함수 | 문제 유형 | 사용 사례 | 특징 |
| :--- | :--- | :--- | :--- |
| **MSE** | 회귀 (Regression) | 주택 가격 예측, 주가 예측 | 예측값과 실제값의 연속적인 오차 측정 |
| **교차 엔트로피** | 분류 (Classification) | 이미지 분류 (고양이/개), 스팸 메일 필터링 | 예측 확률 분포와 실제 분포의 차이 측정 |
| **Focal Loss** | 불균형 분류 | 객체 탐지, 의료 영상 진단 (암 진단 등) | 맞추기 어려운 샘플에 집중하여 학습 |

---

## 4. 예상 면접 질문 (Potential Interview Questions)

*   **Q. 회귀 문제에 교차 엔트로피를 사용하면 안 되는 이유는 무엇인가요?**
    *   **A.** 교차 엔트로피는 확률 분포 간의 차이를 측정하도록 설계되었습니다. 회귀 문제의 출력값은 연속적인 수치이지 확률이 아니므로, 교차 엔트로피를 적용하는 것은 개념적으로 맞지 않습니다. 또한, 회귀 문제에 교차 엔트로피를 억지로 적용하면 손실 함수가 예측 오차를 제대로 반영하지 못해 모델이 올바르게 학습되지 않습니다.

*   **Q. 클래스 불균형 문제가 있을 때 교차 엔트로피 대신 사용할 수 있는 방법은 무엇인가요?**
    *   **A.** Focal Loss를 사용하는 것이 대표적인 해결책입니다. Focal Loss는 모델이 이미 잘 맞추는 다수 클래스의 샘플에 대한 손실은 줄이고, 학습이 어려운 소수 클래스 샘플에 더 가중치를 두어 학습을 진행합니다. 또 다른 방법으로는 클래스 가중치(class weighting)를 교차 엔트로피에 직접 적용하여 소수 클래스의 손실에 더 높은 가중치를 부여하는 방법도 있습니다.

*   **Q. Focal Loss의 파라미터 `gamma`와 `alpha`는 각각 어떤 역할을 하나요?**
    *   **A.** `gamma`는 'Focusing 파라미터'로, 쉬운 샘플과 어려운 샘플 간의 가중치를 조절합니다. `gamma`가 클수록 쉬운 샘플의 손실 기여도가 크게 줄어들어 모델이 어려운 샘플에 더 집중하게 됩니다. `alpha`는 클래스별 가중치 파라미터로, 특정 클래스의 중요도를 직접 조절합니다. 예를 들어, 소수 클래스에 높은 `alpha` 값을 부여하여 해당 클래스의 손실 기여도를 높일 수 있습니다.

---

## 5. 더 읽어보기 (Further Reading)

*   [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)
*   [PyTorch Documentation: Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
*   [A comprehensive guide to loss functions in deep learning](https://neptune.ai/blog/pytorch-loss-functions)