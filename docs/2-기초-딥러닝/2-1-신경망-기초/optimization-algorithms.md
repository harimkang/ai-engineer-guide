---
title: 최적화 알고리즘 (SGD, Momentum, Adam, AdamW)
date: '2025-07-03'
tags: [딥러닝, 신경망, 최적화, SGD, Momentum, Adam, AdamW]
difficulty: medium
---

# 최적화 알고리즘 (SGD, Momentum, Adam, AdamW)

## 1. 핵심 개념 (Core Concept)

최적화 알고리즘(Optimizer)은 손실 함수(Loss Function)의 값을 최소화하기 위해 신경망의 가중치(weights)와 편향(biases)을 어떻게 업데이트할지 결정하는 방법임. 경사 하강법(Gradient Descent)을 기반으로, 더 빠르고 안정적으로 최적의 가중치를 찾기 위한 다양한 변형 알고리즘들이 제안되었음. 대표적으로 SGD, Momentum, Adam, AdamW가 있음.

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

SGD는 가장 기본적인 최적화 알고리즘으로, 전체 데이터가 아닌 미니배치(mini-batch) 단위로 그래디언트를 계산하여 가중치를 업데이트함. 이 방식은 계산 효율성을 높이고, 노이즈가 있는 업데이트 과정이 지역 최적점(local minima)을 탈출하는 데 도움을 줄 수 있음.

- **업데이트 규칙**: `W = W - η * ∇L(W)`
- **장점**: 구현이 간단하고 계산 비용이 낮음.
- **단점**: 손실 공간(loss landscape)이 비등방성(anisotropic)일 경우, 즉 방향에 따라 곡률이 크게 다를 때 탐색 경로가 매우 비효율적이고 수렴 속도가 느림. 학습률(learning rate) 튜닝에 민감함.

### 2.2 모멘텀 (Momentum)

모멘텀은 SGD에 **관성(inertia)** 개념을 도입한 방식임. 이전 그래디언트가 이동했던 방향을 일정 비율 기억하면서 현재 그래디언트와 합쳐 다음 이동 방향을 결정함. 이는 그래디언트의 방향이 일관될 때는 가속을, 방향이 계속 바뀔 때는 진동을 줄여줌.

- **핵심 아이디어**: `v = β * v - η * ∇L(W)`, `W = W + v` (v는 속도, β는 관성 계수)
- **장점**: SGD보다 수렴 속도가 빠르고, 진동 현상을 완화하여 더 안정적으로 최적점을 찾아감.
- **단점**: 하이퍼파라미터(β)를 추가로 튜닝해야 함.

### 2.3 Adam (Adaptive Moment Estimation)

Adam은 각 파라미터마다 \*\*개별적인 학습률(adaptive learning rate)\*\*을 적용하는 방식임. 그래디언트의 1차 모멘텀(평균)과 2차 모멘텀(분산의 제곱근)을 함께 추정하여 학습률을 동적으로 조절함. 이는 방향에 따라 다른 학습 강도가 필요할 때 매우 효과적임.

- **핵심 아이디어**: 그래디언트의 이동 평균과 제곱값의 이동 평균을 모두 사용하여 업데이트 강도를 조절함.
- **장점**: 대부분의 경우 빠르고 안정적인 수렴 성능을 보여주며, 하이퍼파라미터 튜닝에 비교적 덜 민감하여 딥러닝에서 기본 옵티마이저로 널리 사용됨.
- **단점**: 때때로 최적해를 지나치거나 일반화 성능이 SGD+Momentum보다 떨어지는 경우가 보고됨.

### 2.4 AdamW (Adam with Decoupled Weight Decay)

AdamW는 Adam의 **가중치 감쇠(Weight Decay)** 방식을 개선한 알고리즘임. 기존 L2 정규화는 Adam의 적응적 학습률과 결합될 때 특정 가중치가 다른 가중치보다 더 많이 감쇠되는 문제가 있었음. AdamW는 가중치 감쇠를 그래디언트 업데이트 단계에서 분리(decouple)하여 이 문제를 해결함.

- **핵심 아이디어**: 가중치 감쇠를 손실 함수에 추가하는 대신, 옵티마이저의 가중치 업데이트 단계에서 직접 적용함.
- **장점**: Adam보다 더 나은 일반화 성능을 보이는 경우가 많으며, 특히 트랜스포머(Transformer)와 같은 대규모 모델 학습에서 효과적임.
- **단점**: Adam과 마찬가지로 여러 하이퍼파라미터를 가짐.

______________________________________________________________________

## 3. 비교 요약 (Comparison Table)

| 옵티마이저   | 주요 아이디어                   | 장점                 | 단점                    | 주요 사용처                   |
| :----------- | :------------------------------ | :------------------- | :---------------------- | :---------------------------- |
| **SGD**      | 미니배치 그래디언트 사용        | 간단하고 빠름        | 느린 수렴, 진동         | 간단한 모델, 베이스라인       |
| **Momentum** | 이전 그래디언트 방향(관성) 유지 | 빠른 수렴, 진동 완화 | 추가 하이퍼파라미터     | SGD가 느릴 때, CNN            |
| **Adam**     | 파라미터별 적응적 학습률        | 매우 빠르고 안정적   | 일반화 성능 저하 가능성 | 대부분의 딥러닝 모델 (기본값) |
| **AdamW**    | 가중치 감쇠 방식 개선           | 더 나은 일반화 성능  | Adam과 유사한 복잡도    | 트랜스포머 등 대규모 모델     |

______________________________________________________________________

## 4. 예상 면접 질문 (Potential Interview Questions)

- **Q. Adam 옵티마이저가 SGD보다 일반적으로 더 나은 성능을 보이는 이유는 무엇인가요?**

  - **A.** Adam은 각 파라미터마다 개별적인 학습률을 동적으로 조절하기 때문입니다. 손실 공간에서 어떤 방향으로는 변화가 크고 다른 방향으로는 변화가 작을 수 있는데, Adam은 이러한 특성을 감지하여 각 파라미터에 최적화된 스텝 사이즈를 적용합니다. 이는 마치 똑똑한 경사 하강법처럼 작동하여, SGD가 비효율적으로 탐색하는 구간을 훨씬 빠르고 안정적으로 통과할 수 있게 해줍니다.

- **Q. Adam과 AdamW의 가장 큰 차이점은 무엇이며, 왜 AdamW가 제안되었나요?**

  - **A.** 가장 큰 차이점은 가중치 감쇠(Weight Decay)를 처리하는 방식입니다. Adam에서는 가중치 감쇠가 L2 정규화의 일부로 그래디언트에 포함되어, 적응적 학습률의 영향을 받습니다. 이로 인해 학습률이 큰 파라미터는 가중치 감쇠 효과가 줄어드는 문제가 발생합니다. AdamW는 가중치 감쇠를 그래디언트 업데이트에서 분리하여 원래 의도했던 대로 모든 파라미터에 균일하게 정규화 효과를 적용함으로써, 모델의 일반화 성능을 향상시키기 위해 제안되었습니다.

- **Q. 모멘텀(Momentum)은 어떤 원리로 경사 하강법의 수렴 속도를 높이나요?**

  - **A.** 모멘텀은 물리 법칙의 관성과 유사한 원리를 이용합니다. 이전 스텝에서 이동했던 방향과 크기를 기억하고, 현재 스텝의 이동 방향에 그 정보를 더합니다. 만약 그래디언트의 방향이 계속 일관되게 유지된다면, 이동에 가속도가 붙어 더 빠르게 나아갑니다. 반면, 그래디언트 방향이 계속해서 변하는(진동하는) 경우에는 이전 방향과 현재 방향이 서로 상쇄되어 이동 폭이 줄어들기 때문에, 전체적으로는 더 부드럽고 빠르게 최적점으로 수렴하게 됩니다.

______________________________________________________________________

## 5. 더 읽어보기 (Further Reading)

- [Adam: A Method for Stochastic Optimization (Kingma & Ba, 2014)](https://arxiv.org/abs/1412.6980)
- [Decoupled Weight Decay Regularization (Loshchilov & Hutter, 2017)](https://arxiv.org/abs/1711.05101)
- [An overview of gradient descent optimization algorithms (Ruder, 2016)](https://ruder.io/optimizing-gradient-descent/)
