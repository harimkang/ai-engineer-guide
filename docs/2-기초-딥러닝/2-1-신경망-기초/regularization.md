---
title: 정규화 (L1/L2, 드롭아웃, 배치 정규화, 레이어 정규화)
date: '2025-07-03'
tags: [딥러닝, 신경망, 정규화, 과적합, L1, L2, 드롭아웃, 배치 정규화, 레이어 정규화]
difficulty: medium
---

# 정규화 (L1/L2, 드롭아웃, 배치 정규화, 레이어 정규화)

## 1. 핵심 개념 (Core Concept)

정규화(Regularization)는 모델이 학습 데이터에 너무 과도하게 최적화되는 \*\*과적합(Overfitting)\*\*을 방지하고, 새로운 데이터에 대한 \*\*일반화 성능(Generalization)\*\*을 높이기 위한 기법들의 총칭임. 모델의 복잡도에 제약을 가하거나, 학습 과정에 노이즈를 추가하는 등의 방식으로 작동함. 대표적인 기법으로 L1/L2 정규화, 드롭아웃, 배치 정규화, 레이어 정규화가 있음.

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 L1 / L2 정규화 (Weight Decay)

L1/L2 정규화는 손실 함수(Loss Function)에 가중치(weight)의 크기에 대한 패널티 항을 추가하는 방식임. 가중치가 너무 커지는 것을 억제하여 모델을 더 단순하게 만듦.

- **L1 정규화 (Lasso)**: 가중치의 절댓값 합(`Σ|w|`)을 패널티로 사용. 중요하지 않은 특징의 가중치를 0으로 만들어 **특징 선택(Feature Selection)** 효과를 가짐.
- **L2 정규화 (Ridge)**: 가중치의 제곱 합(`Σw²`)을 패널티로 사용. 가중치를 전반적으로 작게 만들어 부드러운 모델을 만듦. 일반적으로 L1보다 더 널리 사용되며, \*\*가중치 감쇠(Weight Decay)\*\*라고도 불림.

### 2.2 드롭아웃 (Dropout)

드롭아웃은 학습 과정에서 각 뉴런을 확률적으로 비활성화(drop)하는 기법임. 매 학습 단계마다 무작위로 일부 뉴런을 제거하여, 신경망이 특정 뉴런에 과도하게 의존하는 것을 방지하고 여러 개의 작은 모델을 앙상블하는 것과 유사한 효과를 냄.

- **작동 방식**: 학습 시에는 지정된 확률(예: p=0.5)로 뉴런의 출력을 0으로 만들고, 추론(inference) 시에는 모든 뉴런을 사용하되, 학습 시 드롭아웃된 비율만큼 출력값을 보정해 줌.
- **효과**: 모델이 더 강건한(robust) 특징을 학습하도록 유도하여 과적합을 효과적으로 방지함.

### 2.3 배치 정규화 (Batch Normalization, BatchNorm)

배치 정규화는 각 층의 활성화(activation) 값의 분포를 미니배치 단위로 정규화(평균 0, 분산 1)하는 기법임. 이는 학습 과정에서 각 층의 입력 분포가 계속 변하는 **내부 공변량 변화(Internal Covariate Shift)** 문제를 완화함.

- **작동 방식**: 미니배치 데이터에 대해 각 특징(feature)별로 평균과 분산을 계산하여 정규화한 후, 학습 가능한 파라미터(gamma, beta)를 통해 다시 스케일링 및 이동시킴.
- **효과**: 학습 속도를 크게 향상시키고, 가중치 초기화에 대한 민감도를 줄이며, 약간의 정규화 효과도 있음.
- **한계**: 미니배치 크기에 의존적이므로, 배치 크기가 매우 작거나 순환 신경망(RNN)처럼 시퀀스 길이가 달라지는 경우 적용하기 어려움.

### 2.4 레이어 정규화 (Layer Normalization, LayerNorm)

레이어 정규화는 배치 정규화의 한계를 극복하기 위해 제안된 기법으로, 미니배치 단위가 아닌 각 데이터 샘플 내의 모든 특징(feature)에 대해 정규화를 수행함.

- **작동 방식**: 각 샘플 내에서 해당 레이어의 모든 뉴런 출력값의 평균과 분산을 계산하여 정규화함.
- **효과**: 배치 크기에 영향을 받지 않아 RNN, 트랜스포머(Transformer)와 같이 시퀀스 데이터를 다루는 모델에 효과적임.

______________________________________________________________________

## 3. 비교 요약 (Comparison Table)

| 기법              | 주요 아이디어                      | 적용 대상   | 장점                                    | 단점/한계                                 |
| :---------------- | :--------------------------------- | :---------- | :-------------------------------------- | :---------------------------------------- |
| **L1/L2**         | 손실 함수에 가중치 패널티 추가     | 가중치      | 구현 간단, 과적합 방지                  | 하이퍼파라미터 튜닝 필요                  |
| **드롭아웃**      | 학습 시 뉴런을 무작위로 비활성화   | 뉴런        | 강력한 정규화 효과, 앙상블 효과         | 학습 시간이 길어질 수 있음                |
| **배치 정규화**   | 미니배치 단위로 활성화 분포 정규화 | 층의 활성화 | 학습 속도 향상, ICS 완화                | 작은 배치 크기에 취약, RNN 적용 어려움    |
| **레이어 정규화** | 샘플 단위로 활성화 분포 정규화     | 층의 활성화 | 배치 크기 무관, RNN/트랜스포머에 효과적 | BatchNorm보다 성능이 떨어지는 경우도 있음 |

______________________________________________________________________

## 4. 예상 면접 질문 (Potential Interview Questions)

- **Q. 과적합(Overfitting)이란 무엇이며, 왜 발생하는지 설명해주세요.**

  - **A.** 과적합은 모델이 학습 데이터에만 너무 잘 맞춰져서, 학습 데이터에 대해서는 성능이 높지만 새로운 데이터(테스트 데이터)에 대해서는 성능이 낮게 나오는 현상을 말합니다. 이는 주로 모델의 복잡도가 데이터의 양에 비해 너무 높거나, 데이터에 노이즈가 많을 때 발생합니다. 모델이 데이터의 실제 패턴뿐만 아니라 노이즈까지 암기해버리기 때문입니다.

- **Q. 드롭아웃이 어떻게 앙상블과 유사한 효과를 내나요?**

  - **A.** 드롭아웃은 학습의 매 스텝마다 다른 뉴런의 조합을 가진, 즉 다른 구조의 서브-네트워크를 학습시키는 것과 같습니다. 이는 마치 수많은 작은 모델들을 각각 학습시키는 것과 유사한 효과를 냅니다. 추론 시에는 모든 뉴런을 사용하는데, 이는 이렇게 학습된 여러 서브-네트워크들의 예측을 평균 내어 종합하는 앙상블 과정과 유사한 결과를 가져와 모델의 일반화 성능을 높입니다.

- **Q. 배치 정규화와 레이어 정규화의 가장 큰 차이점은 무엇이며, 언제 각각을 사용해야 하나요?**

  - **A.** 가장 큰 차이점은 **정규화를 수행하는 단위**입니다. 배치 정규화는 미니배치 내에서 동일한 위치에 있는 특징(feature)들을 모아 정규화하는 반면, 레이어 정규화는 각 데이터 샘플 내에서 해당 레이어의 모든 특징들을 모아 정규화합니다. 따라서 배치 정규화는 CNN과 같이 배치 크기가 충분히 크고 데이터의 통계적 특성이 일정할 때 효과적입니다. 반면, 레이어 정규화는 배치 크기에 영향을 받지 않으므로 RNN이나 트랜스포머처럼 시퀀스 길이가 가변적이거나 배치 크기가 작은 경우에 더 적합합니다.

______________________________________________________________________

## 5. 더 읽어보기 (Further Reading)

- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava et al., 2014)](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe & Szegedy, 2015)](https://arxiv.org/abs/1502.03167)
- [Layer Normalization (Ba et al., 2016)](https://arxiv.org/abs/1607.06450)
