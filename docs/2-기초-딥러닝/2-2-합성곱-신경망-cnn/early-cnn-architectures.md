---
title: 초기 CNN 아키텍처 (LeNet, AlexNet, VGG)
date: '2025-07-03'
tags: [딥러닝, CNN, LeNet, AlexNet, VGG, 아키텍처]
difficulty: medium
---

# 초기 CNN 아키텍처: LeNet, AlexNet, VGG

## 1. 핵심 개념 (Core Concept)

초기 CNN 아키텍처인 LeNet, AlexNet, VGG는 현대 딥러닝 기반 컴퓨터 비전의 발전을 이끈 선구적인 모델들임. LeNet은 CNN의 기본 구조를 제시했고, AlexNet은 깊은 네트워크의 가능성을 입증했으며, VGG는 네트워크의 깊이가 성능에 미치는 중요성을 보여주며 간단하고 표준화된 구조를 제안함. 이 모델들의 발전 과정을 통해 현대 CNN 아키텍처의 핵심 아이디어들이 정립됨.

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 LeNet-5 (1998)

LeNet-5는 Yann LeCun에 의해 개발된 초기 CNN 모델로, 손글씨 숫자 인식(MNIST)과 같은 문서 인식 분야에서 큰 성공을 거둠. 현대 CNN의 기본 구성 요소를 대부분 포함하고 있는 원형으로 평가받음.

- **구조**: 2개의 합성곱 층과 2개의 풀링 층, 그리고 3개의 완전 연결 층으로 구성된 비교적 간단한 7계층 구조.
- **핵심 아이디어**:
  - **합성곱과 풀링의 반복**: 합성곱 층에서 특징을 추출하고, 풀링 층(Subsampling)에서 특징 맵의 크기를 줄이며 위치 변화에 대한 불변성(invariance)을 확보하는 구조를 처음으로 체계화함.
  - **활성화 함수**: 당시에는 시그모이드(Sigmoid)나 하이퍼볼릭 탄젠트(tanh) 함수가 주로 사용됨.

### 2.2 AlexNet (2012)

AlexNet은 2012년 이미지넷(ImageNet) 이미지 인식 대회(ILSVRC)에서 압도적인 성능으로 우승하며 딥러닝의 부흥을 이끈 모델임. LeNet보다 훨씬 더 크고 깊은 네트워크의 성공 가능성을 증명함.

- **구조**: 5개의 합성곱 층과 3개의 완전 연결 층으로 구성된 8계층 구조.
- **핵심 기여**:
  - **ReLU 활성화 함수**: Sigmoid/tanh 대신 ReLU(Rectified Linear Unit)를 사용하여 학습 속도를 크게 향상시키고 기울기 소실 문제를 완화함.
  - **드롭아웃 (Dropout)**: 완전 연결 층에서 드롭아웃을 사용하여 과적합을 효과적으로 방지함.
  - **데이터 증강 (Data Augmentation)**: 이미지 좌우 반전, 일부 영역 잘라내기(cropping) 등 데이터 증강 기법을 적극적으로 사용하여 모델의 일반화 성능을 높임.
  - **GPU 활용**: 2개의 GPU를 병렬로 사용하여 대규모 연산을 효율적으로 처리함.

### 2.3 VGGNet (2014)

VGGNet은 2014년 ILSVRC에서 준우승한 모델로, AlexNet보다 네트워크를 훨씬 더 깊게 쌓으면서도 매우 간단하고 균일한 구조를 제안하여 큰 영향을 미침.

- **구조**: VGG16, VGG19 등 깊이에 따라 버전이 나뉘며, 이름처럼 16개 또는 19개의 가중치 층을 가짐.
- **핵심 아이디어**:
  - **작은 3x3 필터의 반복**: AlexNet의 큰 필터(11x11, 5x5) 대신, 가장 작은 3x3 합성곱 필터만을 반복적으로 쌓아 깊이를 늘림. 3x3 필터를 두 번 쌓으면 5x5 필터와 동일한 수용 영역(receptive field)을 가지면서도, 비선형성을 더 많이 추가하고 파라미터 수를 줄이는 효과가 있음.
  - **균일한 구조**: `(3x3 합성곱 * 2~3회) + (2x2 맥스 풀링)` 형태의 블록을 반복하는 매우 단순하고 표준화된 구조를 가짐. 이 단순성 덕분에 이후 많은 연구에서 베이스라인 모델로 널리 활용됨.

______________________________________________________________________

## 3. 비교 요약 (Comparison Table)

| 모델        | 발표 연도 | 깊이     | 주요 기여 및 특징                                                   |
| :---------- | :-------- | :------- | :------------------------------------------------------------------ |
| **LeNet-5** | 1998      | 7 층     | CNN의 기본 구조 정립 (Conv-Pool-FC)                                 |
| **AlexNet** | 2012      | 8 층     | ReLU, 드롭아웃, 데이터 증강, GPU 활용으로 딥러닝 시대 개막          |
| **VGGNet**  | 2014      | 16/19 층 | 3x3의 작은 필터만 사용하여 깊이의 중요성 증명, 간단하고 균일한 구조 |

______________________________________________________________________

## 4. 예상 면접 질문 (Potential Interview Questions)

- **Q. AlexNet이 딥러닝 역사에서 중요한 모델로 평가받는 이유는 무엇인가요?**

  - **A.** AlexNet은 2012년 이미지넷 대회에서 기존의 머신러닝 방법론들을 압도적인 성능 차이로 이기고 우승하면서, 깊은 신경망(Deep Neural Network)의 실질적인 성능과 가능성을 전 세계에 증명했기 때문입니다. ReLU 함수, 드롭아웃, GPU 활용 등 AlexNet이 도입한 기술들은 이후 딥러닝 모델들의 표준적인 구성 요소가 되었고, 딥러닝 연구의 폭발적인 증가를 촉발하는 계기가 되었습니다.

- **Q. VGGNet이 5x5나 7x7 같은 큰 필터 대신 3x3의 작은 필터를 여러 개 쌓아 사용한 이유는 무엇인가요?**

  - **A.** 두 가지 주요 이점이 있습니다. 첫째, **파라미터 수를 줄일 수 있습니다.** 예를 들어, 5x5 필터 하나는 25개의 파라미터를 갖지만, 3x3 필터 두 개를 쌓으면 3*3 + 3*3 = 18개의 파라미터만으로 동일한 수용 영역을 커버할 수 있습니다. 둘째, **비선형성을 더 많이 추가할 수 있습니다.** 필터 사이에 활성화 함수(ReLU)가 더 많이 들어가기 때문에, 모델의 표현력이 향상되어 더 복잡한 특징을 학습할 수 있습니다.

- **Q. LeNet-5부터 VGGNet까지 이어지는 초기 CNN 아키텍처의 발전 방향을 요약 설명해주세요.**

  - **A.** 초기 CNN 아키텍처는 **더 깊고, 더 효율적인 학습**을 하는 방향으로 발전했습니다. LeNet-5가 합성곱과 풀링이라는 기본 구조를 제시하자, AlexNet은 ReLU와 드롭아웃 같은 새로운 기술을 도입하여 훨씬 더 깊은 네트워크를 성공적으로 학습시켰습니다. 이후 VGGNet은 여기서 더 나아가, 3x3이라는 작은 필터를 반복적으로 쌓는 단순한 방식으로 네트워크의 깊이를 크게 늘려, 깊이가 성능 향상의 핵심 요소임을 보여주었습니다. 즉, '어떻게 하면 네트워크를 더 깊게 쌓고 효과적으로 학습시킬 수 있는가'가 초기 발전의 핵심적인 질문이었습니다.

______________________________________________________________________

## 5. 더 읽어보기 (Further Reading)

- [Gradient-Based Learning Applied to Document Recognition (LeCun et al., 1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) - LeNet-5 원 논문
- [ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) - AlexNet 원 논문
- [Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan & Zisserman, 2014)](https://arxiv.org/abs/1409.1556) - VGGNet 원 논문
