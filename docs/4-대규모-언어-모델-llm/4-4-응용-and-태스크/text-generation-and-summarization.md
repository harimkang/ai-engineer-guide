---
title: 텍스트 생성 및 요약
date: '2025-07-05'
tags: [LLM, Application, Text Generation, Summarization]
difficulty: easy
---

# 텍스트 생성 및 요약

## 1. 핵심 개념 (Core Concept)

\*\*텍스트 생성(Text Generation)\*\*은 LLM이 주어진 프롬프트나 문맥을 바탕으로 인간과 유사한 새로운 텍스트를 만들어내는 핵심 능력입니다. \*\*텍스트 요약(Text Summarization)\*\*은 이 생성 능력을 활용하여 긴 문서의 핵심 내용을 간결하고 일관성 있는 문장으로 압축하는 구체적인 응용 태스크입니다. 이 두 가지는 LLM의 가장 대표적이고 강력한 활용 사례입니다.

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 텍스트 생성 (Text Generation)

LLM은 기본적으로 다음에 올 단어를 예측하는 자기회귀(auto-regressive) 방식으로 텍스트를 생성합니다. 하지만 단순히 가장 확률 높은 단어만 선택하면 단조롭고 반복적인 결과가 나올 수 있습니다. 따라서 더 창의적이고 자연스러운 텍스트를 만들기 위해 다양한 \*\*디코딩 전략(Decoding Strategies)\*\*이 사용됩니다.

- **Greedy Search**: 각 단계에서 가장 확률이 높은 토큰 하나만 선택합니다. 빠르지만 반복적이고 지루한 텍스트를 생성할 수 있습니다.
- **Beam Search**: 각 단계에서 가장 확률이 높은 k개의 후보 시퀀스(beam)를 유지하며 탐색합니다. Greedy Search보다 일관성 있는 문장을 생성하지만, 여전히 다양성이 부족할 수 있습니다.
- **Sampling (Top-K, Top-P)**: 확률 분포에 기반하여 무작위로 다음 토큰을 선택하여 다양성을 높입니다.
  - **Top-K Sampling**: 확률이 가장 높은 K개의 후보 중에서만 샘플링하여 관련 없는 단어가 뽑힐 위험을 줄입니다.
  - **Top-P (Nucleus) Sampling**: 확률의 합이 특정 임계값(p)을 넘는 최소한의 후보 집합(nucleus) 내에서 샘플링합니다. 문맥에 따라 후보군의 크기가 동적으로 변하여 더 자연스러운 결과를 만듭니다.

### 2.2 텍스트 요약 (Text Summarization)

텍스트 요약은 크게 두 가지 접근 방식으로 나뉩니다.

- **추출 요약 (Extractive Summarization)**

  - **방식**: 원문에서 핵심적인 문장이나 구절을 **그대로 추출**하여 요약문을 구성합니다.
  - **장점**: 원문의 내용을 그대로 사용하므로 사실 관계가 왜곡될 위험(hallucination)이 적습니다.
  - **단점**: 문장 간의 연결이 부자연스럽거나, 전체적인 흐름이 매끄럽지 않을 수 있습니다.

- **추상 요약 (Abstractive Summarization)**

  - **방식**: 원문의 핵심 내용을 모델이 **이해하고, 이를 바탕으로 새로운 문장을 생성**하여 요약문을 만듭니다.
  - **장점**: 인간이 작성한 것처럼 자연스럽고 간결하며, 응집력 있는 요약문을 생성할 수 있습니다.
  - **단점**: 모델이 원문에 없는 내용을 지어내거나(hallucination) 중요한 세부 사항을 왜곡할 위험이 있습니다.

대부분의 최신 LLM(BART, T5, GPT 등)은 추상 요약 방식을 사용하여 높은 품질의 요약문을 생성합니다.

______________________________________________________________________

## 3. 예시 (Example)

### 사용 사례

- **뉴스 기사 요약**: 긴 기사를 3줄 요약으로 빠르게 파악.
- **회의록 요약**: 회의 녹취록에서 주요 결정 사항과 담당자를 정리.
- **이메일 초안 작성**: 몇 가지 핵심 키워드를 바탕으로 비즈니스 이메일 초안 생성.
- **창의적 글쓰기**: 소설의 도입부, 시, 마케팅 카피 등 창의적인 텍스트 생성.
- **고객 리뷰 요약**: 수많은 고객 리뷰를 분석하여 제품의 장단점을 요약.

### 긴 문서 처리 기법

LLM은 한 번에 처리할 수 있는 입력 길이(context window)에 한계가 있습니다. 긴 문서를 요약하기 위해 다음과 같은 기법이 사용됩니다.

- **Map-Reduce**: 문서를 여러 개의 작은 덩어리(chunk)로 나누어 각 덩어리를 독립적으로 요약(Map 단계)한 뒤, 이 요약본들을 다시 모아 최종적으로 요약(Reduce 단계)합니다.
- **Refine**: 첫 번째 덩어리를 요약하고, 이 요약본과 다음 덩어리를 함께 입력하여 요약본을 점진적으로 개선해나가는 방식입니다.

______________________________________________________________________

## 4. 예상 면접 질문 (Potential Interview Questions)

- **Q. 텍스트 생성 시 Top-K 샘플링과 Top-P(Nucleus) 샘플링의 차이점은 무엇이며, 어떤 경우에 Top-P가 더 선호될 수 있나요?**

  - **A.** Top-K는 항상 고정된 K개의 후보 중에서 샘플링하는 반면, Top-P는 확률 분포에 따라 후보군의 크기가 동적으로 변합니다. 예를 들어, 모델이 다음에 올 단어를 매우 확신하는 경우(하나의 단어 확률이 99%) Top-P는 후보군을 1개로 줄여 엉뚱한 단어가 생성될 위험을 막아줍니다. 반대로, 확률 분포가 평탄할 때는 더 많은 후보를 고려하여 창의성을 높일 수 있습니다. 이처럼 문맥에 따라 유연하게 대처할 수 있기 때문에 Top-P가 더 선호되는 경우가 많습니다.

- **Q. 추출 요약과 추상 요약의 장단점을 비교 설명해주세요.**

  - **A.** **추출 요약**은 원문을 그대로 사용하므로 사실 왜곡의 위험이 적다는 것이 가장 큰 장점이지만, 요약문의 가독성과 자연스러움이 떨어질 수 있습니다. **추상 요약**은 인간처럼 자연스럽고 간결한 요약을 생성할 수 있다는 장점이 있지만, 모델이 원문에 없는 내용을 지어내는 환각(hallucination) 현상이 발생할 수 있다는 치명적인 단점이 있습니다.

- **Q. LLM을 사용하여 긴 문서를 요약할 때 발생할 수 있는 문제점과 이를 해결하기 위한 접근법(e.g., Map-Reduce)에 대해 설명해주세요.**

  - **A.** 가장 큰 문제점은 LLM의 제한된 컨텍스트 윈도우 때문에 긴 문서 전체를 한 번에 처리할 수 없다는 것입니다. 이를 해결하기 위해 **Map-Reduce** 방식이 사용됩니다. 문서를 여러 청크로 나누어 각 청크를 개별적으로 요약한 후, 이 요약본들을 다시 합쳐 최종 요약을 생성합니다. 이 방식은 병렬 처리가 가능하여 효율적이지만, 청크 간의 전체적인 문맥이 손실될 수 있다는 단점이 있습니다.

______________________________________________________________________

## 5. 더 읽어보기 (Further Reading)

- [Hugging Face Blog: How to generate text: using different decoding methods](https://huggingface.co/blog/how-to-generate)
- [Text Summarization with Large Language Models (Blog Post)](https://www.promptingguide.ai/applications/text-summarization)
- [The Ultimate Guide to Text Summarization with LLMs (Blog Post)](https://txt.co/blog/ultimate-guide-text-summarization)
