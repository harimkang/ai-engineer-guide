---
title: 'LLM 평가 스위트: Perplexity, BLEU/ROUGE, MMLU, HELM, MT-Bench'
date: '2025-07-06'
tags: [LLM, 벤치마크, 평가]
difficulty: medium
---

# LLM 평가 스위트: Perplexity, BLEU/ROUGE, MMLU, HELM, MT-Bench

## 1. 핵심 개념 (Core Concept)

LLM(대규모 언어 모델)의 성능을 다각적으로 측정하고 비교하기 위한 표준화된 지표와 벤치마크 모음입니다. 특정 작업(Task)에서의 성능뿐만 아니라, 언어 이해, 추론, 생성 능력 등 모델의 전반적인 역량을 종합적으로 평가하는 것을 목표로 합니다. 이를 통해 모델의 강점과 약점을 파악하고, 특정 사용 사례에 가장 적합한 모델을 선택하거나 모델 개선 방향을 설정할 수 있습니다.

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 전통적인 언어 모델 평가 지표

#### Perplexity (PPL)

- **개념**: 언어 모델이 특정 텍스트 시퀀스를 얼마나 잘 예측하는지를 측정하는 지표입니다. PPL이 낮을수록 모델이 다음 단어를 더 정확하게 예측한다는 의미이며, 해당 텍스트 분포를 더 잘 학습했음을 시사합니다.
- **계산 방식**: 테스트 세트에 있는 각 단어에 대한 예측 확률의 역수를 기하 평균하여 계산합니다.
- **한계**: 문법적 정확성이나 의미적 일관성보다는 통계적 예측 능력에 초점을 맞추므로, 실제 인간이 느끼는 자연스러움이나 유창함과는 차이가 있을 수 있습니다.

#### BLEU (Bilingual Evaluation Understudy)

- **개념**: 기계 번역 결과물의 품질을 평가하기 위해 제안된 지표로, 모델이 생성한 문장과 사람이 직접 번역한 정답 문장(Reference) 간의 유사도를 측정합니다.
- **계산 방식**: 주로 n-gram(연속된 n개의 단어)의 정밀도(Precision)를 계산하여 점수를 매깁니다. 즉, 생성된 문장의 n-gram이 정답 문장에 얼마나 많이 포함되어 있는지를 봅니다. 짧은 문장에 대한 페널티(Brevity Penalty)를 부여하여 너무 짧은 번역 결과가 높은 점수를 받는 것을 방지합니다.
- **한계**: 의미적 유사성보다는 단어의 일치 여부에 초점을 맞추므로, 다양한 표현을 제대로 평가하기 어렵습니다.

#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

- **개념**: 주로 텍스트 요약(Summarization)의 성능을 평가하는 데 사용되며, 모델이 생성한 요약문이 원본 문서의 핵심 내용을 얼마나 잘 포함하는지를 측정합니다.
- **계산 방식**: BLEU와 유사하게 n-gram을 사용하지만, 정밀도(Precision) 대신 재현율(Recall)에 초점을 맞춥니다. 즉, 정답 요약문의 n-gram이 모델이 생성한 요약문에 얼마나 많이 포함되어 있는지를 측정합니다.

### 2.2 최신 LLM 평가 벤치마크

전통적인 지표들이 LLM의 복합적인 능력을 평가하기에 한계가 드러나면서, 다양한 측면을 종합적으로 평가하는 대규모 벤치마크 스위트가 등장했습니다.

#### MMLU (Massive Multitask Language Understanding)

- **개념**: 57개의 다양한 주제(초등 수학, 미국 역사, 컴퓨터 과학, 법학 등)에 대한 다지선다형 질문을 통해 LLM의 광범위한 지식과 문제 해결 능력을 평가합니다.
- **특징**: 단순한 정보 검색을 넘어, 깊이 있는 지식과 추론 능력을 요구하는 문제들로 구성되어 있어 모델의 "지능"을 측정하는 중요한 척도로 사용됩니다.

#### HELM (Holistic Evaluation of Language Models)

- **개념**: 스탠포드 대학에서 개발한 벤치마크로, 정확성, 공정성, 편향성, 견고성, 효율성 등 7가지 지표를 사용하여 42개의 시나리오에서 모델을 종합적으로 평가합니다.
- **특징**: 단일 점수로 모델을 평가하기보다는, 여러 시나리오와 지표에 걸친 상세한 분석을 제공하여 모델의 다각적인 성능 프로필을 제시합니다. (Paine, 2022)

#### MT-Bench

- **개념**: 실제 사용자들이 LLM에게 할 법한, 정답이 열려 있는(open-ended) 질문에 대한 답변 품질을 평가하는 벤치마크입니다. 주로 챗봇의 대화 능력을 평가하는 데 사용됩니다.
- **평가 방식**: 80개의 질문에 대한 모델의 답변을 더 강력한 LLM(예: GPT-4)을 사용하여 채점하고 순위를 매깁니다. 이를 통해 인간의 선호도와 유사한 평가 결과를 얻고자 합니다.

______________________________________________________________________

## 3. 비교 (Comparison)

| 벤치마크       | 주요 평가 항목                          | 평가 방식                                     | 주요 용도                           |
| :------------- | :-------------------------------------- | :-------------------------------------------- | :---------------------------------- |
| **Perplexity** | 언어 예측의 정확성                      | 통계적 확률 기반                              | 언어 모델의 기본적인 학습 수준 측정 |
| **BLEU/ROUGE** | 생성된 텍스트와 정답 텍스트 간의 유사성 | N-gram 기반 정밀도/재현율                     | 기계 번역, 텍스트 요약              |
| **MMLU**       | 다중 작업 언어 이해 (광범위한 지식)     | 57개 주제에 대한 객관식 문제 해결             | 모델의 지식 및 추론 능력 평가       |
| **HELM**       | 정확성, 공정성, 편향성 등 다차원적 속성 | 42개 시나리오에 대한 7가지 지표 기반 분석     | 모델의 종합적인 성능 및 신뢰성 분석 |
| **MT-Bench**   | 개방형 질문에 대한 대화 능력 및 유용성  | 강력한 LLM을 이용한 상대 평가 (Chatbot Arena) | 챗봇 및 대화형 AI의 성능 비교       |

______________________________________________________________________

## 4. 예상 면접 질문 (Potential Interview Questions)

- **Q. LLM의 성능을 평가할 때 Perplexity만으로 충분하지 않은 이유는 무엇인가요?**

  - **A.** Perplexity는 모델의 통계적 예측 능력, 즉 주어진 텍스트 다음에 올 단어를 얼마나 잘 예측하는지를 측정합니다. 하지만 이는 문법적, 의미적 정확성이나 생성된 문장의 창의성, 논리성, 유용성 등 인간이 중요하게 생각하는 질적 측면을 평가하지 못합니다. 따라서 MMLU, HELM, MT-Bench와 같이 다양한 측면을 종합적으로 평가하는 벤치마크가 필요합니다.

- **Q. 새로운 챗봇 모델을 개발했을 때, 어떤 벤치마크를 사용하여 성능을 평가하는 것이 가장 적절할까요?**

  - **A.** MT-Bench가 가장 적절한 선택일 수 있습니다. MT-Bench는 실제 사용자들이 할 법한 개방형 질문에 대한 모델의 답변 품질과 대화 능력을 직접적으로 평가하기 때문입니다. 여기에 더해 MMLU를 통해 모델의 기본적인 지식 수준을, HELM을 통해 편향성이나 안전성 같은 신뢰성 측면을 함께 평가하여 모델을 다각적으로 검증할 수 있습니다.

- **Q. BLEU와 ROUGE의 근본적인 차이점은 무엇이며, 각각 어떤 작업에 더 적합한가요?**

  - **A.** BLEU는 정밀도(Precision) 기반으로, 모델이 생성한 결과물이 얼마나 정확하게 정답에 포함되는지를 측정합니다. 따라서 번역의 정확성이 중요한 기계 번역에 주로 사용됩니다. 반면 ROUGE는 재현율(Recall) 기반으로, 정답의 핵심 내용이 모델의 생성 결과물에 얼마나 잘 포함되었는지를 측정하므로, 원문의 핵심을 놓치지 않는 것이 중요한 텍스트 요약 평가에 더 적합합니다.

______________________________________________________________________

## 5. 더 읽어보기 (Further Reading)

- [HELM: Holistic Evaluation of Language Models (Official Website)](https://crfm.stanford.edu/helm/latest/)
- [Measuring Massive Multitask Language Understanding (MMLU Paper)](https://arxiv.org/abs/2009.03300)
- [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena (LMSYS Blog)](https://lmsys.org/blog/2023-06-22-leaderboard/)
- [BLEU: a Method for Automatic Evaluation of Machine Translation (ACL Anthology)](https://aclanthology.org/P02-1040.pdf)
