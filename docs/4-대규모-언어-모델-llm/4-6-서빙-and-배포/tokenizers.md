---
title: "토크나이저: BPE, WordPiece, SentencePiece"
date: "2025-07-06"
tags: ["LLM", "서빙", "토크나이저"]
difficulty: "medium"
---

# 토크나이저: BPE, WordPiece, SentencePiece

## 1. 핵심 개념 (Core Concept)

토크나이저(Tokenizer)는 LLM(대규모 언어 모델)이 텍스트를 이해하고 처리할 수 있도록, 원시 텍스트(Raw Text)를 모델의 입력 단위인 '토큰(Token)'으로 분할하는 역할을 합니다. 이 과정은 LLM의 성능과 효율성에 큰 영향을 미치며, 특히 OOV(Out-Of-Vocabulary) 문제(모델이 학습하지 못한 단어 처리)를 해결하는 데 중요합니다. BPE(Byte-Pair Encoding), WordPiece, SentencePiece는 현재 LLM에서 가장 널리 사용되는 서브워드(Subword) 토크나이징 알고리즘입니다.

---

## 2. 상세 설명 (Detailed Explanation)

### 2.1 서브워드 토크나이징의 필요성

*   **단어 기반 토크나이징의 한계**: 모든 단어를 고유한 토큰으로 처리하면 어휘집(Vocabulary) 크기가 너무 커지고, 모델이 학습하지 못한 새로운 단어(OOV)에 대한 처리가 어렵습니다.
*   **문자 기반 토크나이징의 한계**: 모든 문자를 개별 토큰으로 처리하면 어휘집 크기는 작지만, 시퀀스 길이가 너무 길어져 모델의 효율성이 떨어집니다.
*   **서브워드 토크나이징**: 단어와 문자 토크나이징의 장점을 결합한 방식으로, 자주 등장하는 단어는 하나의 토큰으로, 드물게 등장하는 단어나 OOV 단어는 의미 있는 서브워드(예: 'un', '##able')로 분할하여 처리합니다. 이를 통해 어휘집 크기를 적절히 유지하면서 OOV 문제를 효과적으로 해결합니다.

### 2.2 Byte-Pair Encoding (BPE)

*   **개념**: 데이터 압축 알고리즘에서 유래했습니다. 가장 빈번하게 등장하는 연속된 문자 쌍(또는 토큰 쌍)을 하나의 새로운 토큰으로 병합하는 과정을 반복하여 어휘집을 구축합니다.
*   **동작 방식**:
    1.  초기 어휘집은 모든 개별 문자로 구성됩니다.
    2.  학습 데이터에서 가장 자주 나타나는 인접한 문자 쌍을 찾아 새로운 토큰으로 병합하고 어휘집에 추가합니다.
    3.  이 과정을 미리 정해진 횟수만큼 반복하거나, 목표 어휘집 크기에 도달할 때까지 수행합니다.
*   **주요 사용 모델**: GPT 시리즈, Llama, Mistral 등 많은 LLM에서 사용됩니다.

### 2.3 WordPiece

*   **개념**: BPE와 유사하지만, 병합 기준이 다릅니다. 가장 빈번한 쌍을 병합하는 대신, 병합했을 때 학습 데이터의 '언어 모델 우도(likelihood)'를 가장 크게 증가시키는 쌍을 선택하여 병합합니다.
*   **동작 방식**: 각 병합 단계에서, 병합될 경우 전체 훈련 코퍼스의 우도를 가장 크게 증가시키는 서브워드 쌍을 선택합니다. 이는 통계적 모델링에 기반하여 더 의미 있는 서브워드를 생성하는 데 유리합니다.
*   **주요 사용 모델**: Google의 BERT, DistilBERT 등에서 사용됩니다.

### 2.4 SentencePiece

*   **개념**: 입력 텍스트를 공백을 포함한 '원시 문자 스트림'으로 간주하여 토크나이징을 수행하는 언어 독립적인(Language-agnostic) 토크나이저입니다. 특히 중국어, 일본어와 같이 단어 구분이 명확하지 않은 언어에 유용합니다.
*   **주요 특징**:
    *   **사전 토크나이징 불필요**: 텍스트를 미리 단어 단위로 분리할 필요 없이 원시 텍스트에 직접 적용됩니다.
    *   **공백 처리**: 공백을 일반 문자처럼 토큰화 과정에 포함시켜, 원본 텍스트를 완벽하게 복원할 수 있는 '손실 없는(Lossless)' 토크나이징을 가능하게 합니다.
    *   **다양한 알고리즘 지원**: BPE와 유사한 빈도 기반 병합 알고리즘뿐만 아니라, 유니그램(Unigram) 언어 모델 기반의 분할 알고리즘도 지원합니다.
*   **주요 사용 모델**: T5, mBERT, XLNet 등 다국어 모델에서 주로 사용됩니다.

---

## 3. 비교 (Comparison)

| 특징           | BPE (Byte-Pair Encoding)             | WordPiece                                  | SentencePiece                               |
| :------------- | :----------------------------------- | :----------------------------------------- | :------------------------------------------ |
| **핵심 아이디어** | 가장 빈번한 인접 토큰 쌍 병합        | 언어 모델 우도를 최대화하는 토큰 쌍 병합   | 원시 텍스트를 문자 스트림으로 처리          |
| **사전 토크나이징** | 일반적으로 필요                      | 필요                                       | 불필요 (언어 독립적)                        |
| **공백 처리**  | 사전 토크나이징 과정에서 처리됨      | 특수 접두사(예: `##`)로 처리되거나 손실될 수 있음 | 일반 문자처럼 토큰화에 포함됨 (예: ` `)     |
| **주요 사용처** | GPT, Llama, Mistral                  | BERT, DistilBERT                           | T5, mBERT, XLNet                            |
| **장점**       | 구현이 간단하고 효과적               | 언어 모델 성능에 최적화된 어휘집 생성      | 다국어 지원, 원본 텍스트 완벽 복원 가능     |

---

## 4. 예상 면접 질문 (Potential Interview Questions)

*   **Q. 토크나이저가 LLM의 성능에 미치는 영향은 무엇인가요?**
    *   **A.** 토크나이저는 LLM의 입력 단위를 결정하므로, 모델의 어휘집 크기, 시퀀스 길이, 그리고 OOV(Out-Of-Vocabulary) 문제 해결 능력에 직접적인 영향을 미칩니다. 적절한 토크나이저는 어휘집 크기를 효율적으로 관리하여 모델의 학습 및 추론 효율성을 높이고, OOV 단어를 의미 있는 서브워드로 분할하여 모델이 새로운 단어나 복잡한 단어를 더 잘 이해하고 생성할 수 있도록 돕습니다. 잘못된 토크나이저는 정보 손실, 불필요한 시퀀스 길이 증가, 또는 OOV 문제로 인해 모델 성능 저하를 야기할 수 있습니다.

*   **Q. 한국어와 같이 어절 구분이 명확하지 않은 언어에 SentencePiece가 특히 유리한 이유는 무엇인가요?**
    *   **A.** 한국어는 '은/는/이/가'와 같은 조사가 단어에 붙어 형태가 변하거나, 띄어쓰기가 불규칙한 경우가 많아 단어 경계를 명확히 구분하기 어렵습니다. SentencePiece는 텍스트를 미리 단어 단위로 분리하지 않고, 공백을 포함한 원시 문자 스트림에서 직접 서브워드를 학습하고 생성합니다. 이 방식은 언어의 특성에 관계없이 일관된 토크나이징을 가능하게 하여, 한국어처럼 복잡한 언어에서도 효과적으로 토큰을 생성하고 OOV 문제를 해결하는 데 유리합니다.

*   **Q. BPE와 WordPiece의 주요 차이점은 무엇이며, 각각 어떤 상황에 더 적합하다고 생각하시나요?**
    *   **A.** BPE는 가장 빈번하게 등장하는 문자 쌍을 병합하는 '빈도 기반' 방식인 반면, WordPiece는 병합했을 때 학습 데이터의 '언어 모델 우도(likelihood)'를 가장 크게 증가시키는 쌍을 선택하는 '통계 기반' 방식입니다. BPE는 구현이 간단하고 직관적이며, 다양한 언어에 범용적으로 적용하기 좋습니다. WordPiece는 언어 모델의 성능을 직접적으로 최적화하는 데 더 유리하므로, 특정 언어 모델의 성능을 극대화하고자 할 때 더 적합할 수 있습니다. 예를 들어, BERT와 같이 언어 이해에 중점을 둔 모델에서는 WordPiece가 더 좋은 성능을 보였습니다.

---

## 5. 더 읽어보기 (Further Reading)

*   [Hugging Face - Tokenizers Explained](https://huggingface.co/docs/transformers/tokenizer_summary)
*   [SentencePiece: A simple and language-independent subword tokenizer and detokenizer for Neural Text Processing (Paper)](https://arxiv.org/abs/1808.07012)
*   [BPE: Neural Machine Translation of Rare Words with Subword Units (Paper)](https://arxiv.org/abs/1508.07909)