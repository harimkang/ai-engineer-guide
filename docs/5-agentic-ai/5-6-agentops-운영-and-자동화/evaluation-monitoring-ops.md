---
title: '평가 및 모니터링 (운영): 궤적, 텔레메트리, 대시보드'
date: '2025-10-29'
tags: [Agentic-AI, AgentOps, Monitoring, Evaluation, Observability]
difficulty: hard
---

# 평가 및 모니터링 (운영 관점)

## 1. 핵심 개념 (Core Concept)

에이전트의 운영(Ops)에서 평가는 \*\*오프라인 평가(품질 보증)\*\*와 **온라인 모니터링(상태 확인)** 이라는 두 축으로 이루어집니다. 오프라인 평가는 배포 전 '골든 데이터셋'으로 품질을 검증하는 것이고, 온라인 모니터링은 배포 후 실제 트래픽에 대해 **품질(Quality), 비용(Cost), 지연 시간(Latency)** 이라는 세 가지 핵심 지표를 추적하여 서비스 수준 목표(SLO)를 충족하는지 지속적으로 감시하는 활동입니다. 이 두 가지가 결합되어야 안정적이고 신뢰할 수 있는 에이전트 운영이 가능합니다.

*Note: 아래 다이어그램을 위한 이미지를 `docs/images/agent-monitoring-loop.png` 에 추가해주세요.*
![Agent Monitoring Loop Diagram](../../images/agent-monitoring-loop.png)

______________________________________________________________________

## 2. 상세 설명 (Detailed Explanation)

### 2.1 오프라인 평가 vs. 온라인 모니터링

| 구분          | 오프라인 평가 (배포 전)                                 | 온라인 모니터링 (배포 후)                                                 |
| :------------ | :------------------------------------------------------ | :------------------------------------------------------------------------ |
| **목표**      | **품질 게이트**: 새 버전이 배포될 만큼 좋은가?          | **건강 체크**: 라이브 시스템이 정상적으로 동작하는가?                     |
| **데이터**    | 고정된 '골든 데이터셋'                                  | 예측 불가능한 실제 사용자 트래픽                                          |
| **주요 지표** | **품질 중심**: LLM-as-a-Judge 점수, F1, ROUGE           | **운영 중심**: 에러율, p99 지연 시간, 요청당 비용                         |
| **방법론**    | **회귀 테스트**: 이전 버전과 새 버전의 점수를 직접 비교 | **A/B 테스트 및 이상 탐지**: 카나리/컨트롤 그룹 비교, 지표 급증/급락 감시 |
| **결과물**    | 배포에 대한 **Go / No-Go 결정**                         | **대시보드와 알림**, 그리고 인시던트 대응                                 |

### 2.2 모니터링의 삼위일체: 품질, 비용, 속도

운영 환경에서는 아래 세 가지 핵심 지표를 중심으로 대시보드를 구성하고 서비스 수준 목표(SLO)를 설정해야 합니다.

1. **품질 (Quality)**: 에이전트가 제 역할을 올바르게 수행하고 있는가?
   - **지표**: 작업 성공률, LLM-as-a-Judge 점수, 사용자 피드백(좋아요/싫어요), 환각 발생률, 근거 기반 답변 비율(Groundedness).
1. **비용 (Cost)**: 에이전트를 운영하는 데 얼마의 비용이 드는가?
   - **지표**: 요청당 평균 비용, 시간당 총비용, 요청당 평균 토큰 사용량.
1. **속도 (Latency)**: 사용자가 답변을 받기까지 얼마나 기다려야 하는가?
   - **지표**: 종단간(End-to-end) p95/p99 지연 시간, 각 도구 호출 및 LLM 호출의 개별 지연 시간.

### 2.3 궤적 기반 평가 (Trajectory-based Evaluation)

에이전트의 최종 답변만 평가하는 것은 한계가 있습니다. **궤적(Trajectory)**, 즉 에이전트의 `Thought → Action → Observation` 로그 전체를 평가 단위로 삼아야, '어느 단계'에서 문제가 발생했는지 근본 원인을 파악할 수 있습니다. 예를 들어, 최종 답변이 틀렸더라도 그 원인이 'LLM의 추론'이 아닌 '도구의 실패' 때문일 수 있습니다.

- **추적 도구**: OpenTelemetry, LangSmith, PromptLayer 등의 도구를 사용하여 에이전트의 모든 내부 동작을 스팬(span)으로 기록하고 시각화합니다.
- **분석**: `trace_id`를 통해 특정 요청의 전체 궤적을 재현(replay)하여 디버깅하고, 집계된 궤적 데이터를 통해 '가장 자주 실패하는 도구'나 '가장 시간이 오래 걸리는 추론 단계' 같은 병목 지점을 찾아냅니다.

______________________________________________________________________

## 3. 예상 면접 질문 및 모범 답안

### Q1. 에이전트 운영 지표 중 우선순위를 어떻게 정해야 하나요?

**A.** **안정성 → 품질 → 효율성** 순으로 우선순위를 정해야 합니다. 서비스가 자주 다운되거나 에러를 뿜는다면(안정성), 답변의 품질을 논할 수 없습니다. 또한, 답변의 품질이 엉망이라면(품질), 아무리 빠르고 저렴해도(효율성) 아무 소용이 없습니다. 이 세 가지는 피라미드 구조와 같습니다.

**\[추가 설명\]**

1. **1순위: 안정성 (Reliability)**: 시스템의 생존과 직결되는 지표입니다. (예: Uptime, 에러율, 도구 API 성공률). 이 지표에 문제가 생기면 즉시 대응해야 하는 최우선 순위의 경보(Alert)를 설정해야 합니다.
1. **2순위: 품질 (Quality)**: 서비스의 핵심 가치를 나타내는 지표입니다. (예: 작업 성공률, Judge 점수, 사용자 만족도). 품질 저하는 심각한 문제로, 롤백을 포함한 즉각적인 조사가 필요합니다.
1. **3순위: 효율성 (Efficiency)**: 비즈니스 지속성과 사용자 경험에 영향을 주는 지표입니다. (예: 요청당 비용, p95 지연 시간). 안정성과 품질이 확보된 후에 최적화하는 것이 일반적입니다. 단, 비용이나 지연 시간이 갑자기 급증하는 것은 안정성 문제일 수 있으므로 높은 우선순위의 경보가 필요합니다.

### Q2. 온라인 평가와 오프라인 평가는 어떻게 역할을 분담해야 하나요?

**A.** **오프라인 평가**는 배포 전 '품질 보증(QA)'의 역할을, \*\*온라인 평가(모니터링)\*\*는 배포 후 '운영 상태 확인(Health Check)'의 역할을 담당합니다. 오프라인 평가는 고정된 데이터셋으로 '새 버전이 기존 버전보다 나은지'를 검증하고, 온라인 평가는 실제 트래픽을 통해 '시스템이 안정적으로 동작하는지'를 감시합니다.

**\[추가 설명\]**

- **오프라인 평가 (차고에서의 사전 점검)**: '골든 데이터셋'이라는 고정된 문제지를 풀어보게 하여, 새 버전이 기존 버전보다 점수가 높은지, 그리고 이전에 맞췄던 문제를 틀리지는 않는지(회귀 테스트) 확인합니다. 배포 여부를 결정하는 '게이트' 역할을 합니다.
- **온라인 모니터링 (실제 경주 중의 계기판)**: 실제 도로(사용자 트래픽)를 달릴 때의 속도, 엔진 온도, 에러 발생 여부 등을 실시간으로 확인합니다. A/B 테스트를 통해 실제 사용자들의 반응을 측정하고, 이상 징후(Anomaly)를 탐지하여 사고를 예방하는 역할을 합니다.

### Q3. 비용 급증이나 지연 시간 악화에 대응하기 위한 자동화된 방어 전략은 무엇인가요?

**A.** 가장 중요한 자동화 방어 전략은 **1) 요청 제한(Rate Limiting), 2) 예산 기반 서킷 브레이커, 3) 자동화된 모델 폴백(Fallback)** 입니다. 요청 제한으로 과도한 트래픽을 막고, 서킷 브레이커로 특정 요청의 비용 폭주를 막으며, 폴백으로 주력 모델에 문제가 생겼을 때 차선책으로 서비스를 유지합니다.

**\[추가 설명\]**

- **비용 급증 방어**:
  - **예산 기반 서킷 브레이커**: 특정 에이전트 작업이 비정상적으로 많은 도구를 호출하거나 긴 추론 루프에 빠졌을 때, 실시간으로 비용을 계산하여 예산을 초과하면 해당 작업을 강제로 중단시킵니다.
  - **모델 폴백**: 주력으로 사용하는 비싼 LLM의 비용이 급증할 경우, 일시적으로 더 저렴한 모델로 자동 전환하여 서비스를 유지합니다.
- **지연 시간 악화 방어**:
  - **엄격한 타임아웃**: 모든 외부 호출(LLM, 도구 API)에 대해 합리적인 시간제한을 설정하는 것이 가장 기본적이고 중요합니다.
  - **지연 시간 기반 폴백**: 특정 LLM이나 도구의 응답 시간이 p99 기준 10초를 초과하는 등, SLO를 위반하면 더 빠른 대체 모델이나 도구로 트래픽을 자동 전환합니다.
  - **부하 제어 (Load Shedding)**: 시스템 전체의 지연 시간이 증가하기 시작하면, 시스템 과부하의 신호일 수 있습니다. 이때는 들어오는 요청의 일부를 의도적으로 거부(shedding)하여, 시스템이 안정적인 지연 시간을 회복하도록 합니다.

______________________________________________________________________

## 4. See also

- [프롬프트 및 에이전트 평가 (품질 관점)](../5-5-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-and-%ED%8F%89%EA%B0%80/prompt-evaluation-and-benchmarks.md)
- [에이전트 라이프사이클 운영](../5-6-agentops-%EC%9A%B4%EC%98%81-and-%EC%9E%90%EB%8F%99%ED%99%94/agent-lifecycle-ops.md)
