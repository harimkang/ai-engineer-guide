---
title: '파인튜닝 & 적응: PEFT(LoRA/QLoRA), SFT, DPO'
date: '2025-10-29'
tags: [Agentic-AI, LLM, Fine-tuning, LoRA, DPO]
difficulty: hard
---

# 파인튜닝 & 적응

## 1. 핵심 개념 (Core Concept)

파인튜닝은 사전 학습된(Pre-trained) LLM을 특정 도메인이나 작업에 맞게 **'미세 조정'** 하는 과정입니다. 이는 모델에게 **새로운 지식이나 기술을 가르치거나, 특정한 말투나 출력 형식을 따르도록 스타일을 교정**하는 것을 목표로 합니다. 최근에는 전체 모델을 재학습시키는 대신, **PEFT(Parameter-Efficient Fine-Tuning)**, 특히 LoRA/QLoRA 기법을 사용하여 훨씬 적은 비용과 시간으로 효율적인 파인튜닝을 수행하는 것이 표준으로 자리 잡고 있습니다.

*Note: 아래 다이어그램을 위한 이미지를 `docs/images/lora-finetuning-diagram.png` 에 추가해주세요.*
![LoRA vs Full Fine-tuning Diagram](../../images/lora-finetuning-diagram.png)

______________________________________________________________________

## 2. 파인튜닝의 스펙트럼

모델을 특정 작업에 적응시키는 방법은 비용과 성능에 따라 여러 단계로 나눌 수 있습니다.

| 방법론                  | 학습 대상                         | 비용/시간 | 성능/범용성 | 주요 사용 사례                                   |
| :---------------------- | :-------------------------------- | :-------- | :---------- | :----------------------------------------------- |
| **프롬프트 엔지니어링** | 없음                              | 매우 낮음 | 낮음        | 간단한 스타일 변경, 일회성 작업                  |
| **PEFT (LoRA/QLoRA)**   | 작은 어댑터(전체 파라미터의 \<1%) | 낮음      | 중간        | 특정 도메인 적응, 스타일 교정, 소규모 데이터셋   |
| **전체 파인튜닝**       | 모델의 모든 파라미터              | 매우 높음 | 높음        | 새로운 핵심 역량(예: 코딩) 학습, 대규모 데이터셋 |

______________________________________________________________________

## 3. 상세 설명 (Detailed Explanation)

### 3.1 PEFT: LoRA와 QLoRA

- **LoRA (Low-Rank Adaptation)**: 모델의 거대한 가중치 행렬(W)을 직접 수정하는 대신, 그 옆에 훨씬 작은 두 개의 행렬(A, B)을 추가하고 이 작은 행렬들만 학습시킵니다. 원본 모델은 그대로 두기 때문에(frozen), 적은 메모리로 빠르게 학습할 수 있고, 학습된 어댑터만 교체하며 여러 작업을 수행할 수 있습니다.
- **QLoRA (Quantized LoRA)**: LoRA를 한 단계 더 발전시킨 기법으로, 원본 모델의 가중치를 4비트 정밀도로 양자화(Quantization)하여 메모리 사용량을 극단적으로 줄입니다. 덕분에 70B급의 매우 큰 모델도 단일 소비자용 GPU(예: RTX 4090)에서 파인튜닝할 수 있게 되었습니다.

### 3.2 학습 방식: SFT vs. DPO

파인튜닝의 목표에 따라 다른 학습 방식을 사용합니다.

| 학습 방식                  | 무엇을 가르치는가?                                                          | 데이터 형식                         | 비유                                                                   |
| :------------------------- | :-------------------------------------------------------------------------- | :---------------------------------- | :--------------------------------------------------------------------- |
| **SFT (지도 파인튜닝)**    | **기술과 지식**: 특정 질문에 어떻게 답해야 하는지, 정해진 형식을 따르는 법. | `(지시, 정답)` 쌍                   | 학생에게 문제와 정답지를 주고 외우게 하는 것.                          |
| **DPO (직접 선호 최적화)** | **선호와 스타일**: 더 '인간적인', '유용한', '안전한' 방식으로 답하는 법.    | `(지시, 선호 답변, 비선호 답변)` 쌍 | 학생에게 두 개의 모범 답안을 보여주고 어떤 것이 더 좋은지 알려주는 것. |

- **SFT (Supervised Fine-Tuning)**: '정답'이 명확한 데이터셋을 사용하여 모델이 특정 작업을 수행하는 방법을 배우도록 합니다.
- **DPO (Direct Preference Optimization)**: RLHF의 복잡한 강화학습 과정 없이, 인간이 선호하는 답변과 선호하지 않는 답변 쌍을 이용하여 모델이 직접적으로 인간의 선호를 학습하게 하는 기법입니다. 안정적이고 효율적이어서 최근 널리 사용됩니다.

### 3.3 데이터 전략 및 과적합 방지

- **데이터 큐레이션**: 고품질의 데이터가 파인튜닝의 성패를 좌우합니다. 중복을 제거하고, 개인정보를 마스킹하며, 특정 주제에 편향되지 않도록 데이터를 잘 정제해야 합니다.
- **데이터 혼합**: 특정 도메인 데이터로만 학습시키면 모델이 일반 상식을 잊어버리는 '파국적 망각'이 발생할 수 있습니다. 이를 방지하기 위해, 일반 데이터와 도메인 데이터를 8:2 또는 9:1 비율로 섞어 학습시키는 것이 일반적입니다.
- **과적합 방지**: 조기 종료(Early Stopping), 드롭아웃(Dropout) 등의 정규화 기법을 사용하여 모델이 학습 데이터에만 과도하게 최적화되는 것을 방지합니다.

______________________________________________________________________

## 4. 예상 면접 질문 및 모범 답안

### Q1. DPO의 장점과 주의점은 무엇인가요?

**A.** DPO의 가장 큰 장점은 별도의 보상 모델 학습이나 복잡한 강화학습 파이프라인 없이, **선호도 데이터만으로 직접 모델을 안정적으로 학습**시킬 수 있다는 점입니다. 하지만 **데이터 품질에 매우 민감**하다는 점을 주의해야 합니다. 선호도 데이터에 존재하는 편향(예: 더 긴 답변을 선호하는 경향)을 모델이 그대로 학습하기 때문입니다.

**\[추가 설명\]**

- **장점**: 기존 RLHF 대비 학습 과정이 단순하고 안정적이며, 계산 비용이 저렴합니다.
- **주의점**: DPO는 '스타일'과 '선호'를 가르치는 것이지, '사실'을 가르치는 것이 아닙니다. 만약 선호된 답변이 사실적으로 틀렸다면, 모델은 틀린 답변을 하도록 학습될 것입니다. 따라서 데이터셋을 구축할 때 사실 관계 검증 및 편향성 검토가 매우 중요합니다.

### Q2. QLoRA는 어떤 조건에서 유리하며, 그 한계는 무엇인가요?

**A.** QLoRA는 **사용 가능한 GPU 메모리가 제한적일 때** 압도적으로 유리합니다. 70B와 같은 거대 모델을 단일 소비자용 GPU에서 학습시킬 수 있게 하여, 모델 커스터마이징의 장벽을 크게 낮췄습니다. 하지만 가중치를 4비트로 양자화하는 과정에서 발생하는 **미세한 성능 저하**가 그 한계입니다.

**\[추가 설명\]**

- **유리한 조건**: A100/H100 같은 고가의 GPU 클러스터 없이, 제한된 하드웨어 환경에서 거대 언어 모델을 파인튜닝하고자 할 때 거의 유일한 선택지입니다.
- **한계**: 양자화로 인한 정밀도 손실 때문에, 16비트로 학습하는 일반 LoRA나 전체 파인튜닝에 비해 이론적으로 성능이 약간 저하될 수 있습니다. 대부분의 작업에서는 이 차이가 미미하지만, 극도의 정밀도를 요구하는 과학 계산 등의 작업에서는 문제가 될 수 있습니다.

### Q3. 파인튜닝 시, 일반 데이터와 도메인 특화 데이터의 혼합 비율은 어떻게 정하는 것이 좋은가요?

**A.** 목표에 따라 다릅니다. 모델의 **일반적인 능력을 유지하면서 새로운 도메인에 적응**시키는 것이 목표라면 **일반 데이터 80~90%, 도메인 데이터 10~20%** 비율로 시작하는 것이 좋습니다. 반면, 특정 도메인에 **완전히 특화된 전문가 모델**을 만드는 것이 목표라면 도메인 데이터의 비율을 50% 이상으로 높이는 것을 고려할 수 있습니다. 최적의 비율은 여러 비율로 실험 모델을 만들어, 일반 성능과 도메인 성능을 모두 평가하여 결정해야 합니다.

**\[추가 설명\]**
도메인 데이터로만 학습하면 모델이 기존에 알던 상식이나 대화 능력을 잊어버리는 '파국적 망각(Catastrophic Forgetting)' 현상이 발생할 수 있습니다. 데이터 혼합은 이를 방지하기 위한 필수적인 전략입니다. 최적의 비율을 찾기 위해서는 1) 일반 능력 평가 벤치마크(예: MMLU)와 2) 도메인 특화 평가 데이터셋을 모두 구축하고, 각 비율로 학습된 모델의 성능 변화 추이를 비교하여 목표에 가장 부합하는 트레이드오프 지점을 선택해야 합니다.

### Q4. LoRA 어댑터를 병합(Merge)하거나 스태킹(Stacking)하는 것의 장단점은 무엇인가요?

**A.** \*\*병합(Merging)\*\*은 베이스 모델에 어댑터 가중치를 합쳐 단일 모델로 만드는 것으로, **서빙 시의 지연 시간이 없고 배포가 단순**해지는 장점이 있지만, 모듈성(분리 가능성)을 잃는 단점이 있습니다. \*\*스태킹(Stacking)\*\*은 여러 어댑터를 동적으로 조합하여 사용하는 것으로, **유연성이 극대화**되지만, 추론 시 여러 어댑터를 거치면서 **복잡성과 지연 시간이 증가**하는 단점이 있습니다.

**\[추가 설명\]**

- **병합**: 파인튜닝이 완료된 후, 서빙 환경에 배포할 때 주로 사용됩니다. 추론 속도가 중요하고, 더 이상 어댑터를 교체할 필요가 없을 때 유용합니다.
- **스태킹**: '파이썬 코딩 능력' 어댑터와 '친절한 대화 스타일' 어댑터를 동시에 적용하는 것처럼, 여러 능력을 동적으로 조합하고 싶을 때 사용합니다. 매우 유연하지만, 여러 어댑터 간의 상호작용이 예상치 못한 결과를 낳을 수 있고, 추론 로직이 복잡해지는 등 아직 연구가 활발히 진행 중인 분야입니다.

______________________________________________________________________

## 5. See also

- [프롬프트 평가 및 게이팅](../5-5-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-and-%ED%8F%89%EA%B0%80/prompt-evaluation-and-benchmarks.md)
- [에이전트 라이프사이클 및 배포](../5-6-agentops-%EC%9A%B4%EC%98%81-and-%EC%9E%90%EB%8F%99%ED%99%94/agent-lifecycle-ops.md)
