---
title: '추론 최적화 & 서빙: 양자화, vLLM, TGI, Ollama'
date: '2025-10-29'
tags: [Agentic AI, LLM, Serving, Optimization, vLLM]
difficulty: hard
---

# 추론 최적화 & 서빙

## 1. 핵심 개념 (Core Concept)

LLM 추론 최적화는 한정된 GPU 자원으로 **더 빠른 응답(지연 시간, Latency)을 제공**하고, **더 많은 사용자를 동시에 처리(처리량, Throughput)** 하기 위한 기술입니다. 이는 크게 **1) 메모리 사용량 절감**, **2) GPU 활용률 극대화**, **3) 순수 계산량 감소**라는 세 가지 전략으로 나뉩니다. 양자화, 연속 배칭, PagedAttention, Speculative Decoding 등의 기법을 vLLM, TGI 같은 서빙 프레임워크를 통해 적용하여, 비용 효율적인 LLM 서비스를 구축하는 것이 최종 목표입니다.

______________________________________________________________________

## 2. LLM 추론의 병목: 메모리와 어텐션

- **메모리**: 추론 시 GPU 메모리는 주로 **모델 가중치**와 **KV 캐시**가 차지합니다. 특히 KV 캐시는 사용자의 요청(시퀀스) 길이에 비례하여 커지므로, 동시 사용자가 많아지면 메모리 부족(OOM)의 주된 원인이 됩니다.
- **계산**: 첫 프롬프트를 처리하는 'Prefill' 단계에서는 어텐션 연산(토큰 수의 제곱에 비례)이, 다음 토큰을 생성하는 'Decoding' 단계에서는 GPU 메모리 대역폭이 병목이 됩니다.

______________________________________________________________________

## 3. 핵심 최적화 기법

### 3.1 메모리 사용량 절감

- **양자화 (Quantization)**: 모델의 가중치를 표현하는 데이터 타입의 정밀도를 낮추는(예: 16비트 부동소수점 → 4비트 정수) 기술입니다. 메모리 사용량을 1/2 ~ 1/4까지 줄일 수 있습니다. GPTQ, AWQ 등 정확도 손실을 최소화하는 여러 기법이 있습니다.
- **PagedAttention (vLLM)**: 운영체제의 페이징(Paging) 기법처럼, 조각난 KV 캐시를 물리적으로 연속되지 않은 메모리 공간에 효율적으로 할당합니다. 이를 통해 메모리 단편화를 90% 이상 줄여, 같은 메모리로 더 많은 요청을 처리할 수 있습니다.

### 3.2 GPU 활용률 극대화 (병렬 처리)

- **연속 배칭 (Continuous Batching)**: 기존의 정적 배칭은 배치 내 모든 요청이 끝날 때까지 GPU가 대기해야 했습니다. 연속 배칭은 하나의 요청이 끝나면 그 즉시 새로운 요청을 배치에 추가하여, GPU의 유휴 시간을 최소화하고 처리량을 최대 20배 이상 높입니다.
- **텐서/파이프라인 병렬화**: 단일 모델이 너무 커서 GPU 하나에 올라가지 않을 때, 모델을 여러 조각으로 쪼개 여러 GPU에 분산하여 처리하는 기술입니다.

*Note: 아래 다이어그램을 위한 이미지를 `docs/images/continuous-batching-diagram.png` 에 추가해주세요.*
![Continuous Batching Diagram](../../images/continuous-batching-diagram.png)

### 3.3 순수 계산량 감소

- **Speculative Decoding**: 작고 빠른 '초안(Draft)' 모델이 다음 토큰 몇 개를 예측하면, 크고 정확한 '메인' 모델이 그 예측을 한 번에 검증하는 방식입니다. 초안 모델의 예측이 대부분 맞을 경우, 토큰 생성 속도(지연 시간)를 2~3배 향상시킬 수 있습니다.

______________________________________________________________________

## 4. 서빙 프레임워크 비교

| 프레임워크       | 핵심 기능                 | 주요 사용 사례                                  | 장단점                                                                              |
| :--------------- | :------------------------ | :---------------------------------------------- | :---------------------------------------------------------------------------------- |
| **vLLM**         | PagedAttention, 연속 배칭 | **최대 처리량**이 중요한 범용 고성능 서빙       | 처리량 극대화에 강점. OpenAI API와 호환성이 좋음.                                   |
| **TGI**          | 연속 배칭, 텐서 병렬화    | **안정적인 프로덕션** 서빙, Hugging Face 생태계 | 기능이 풍부하고 안정적이지만, vLLM 대비 처리량은 다소 낮을 수 있음.                 |
| **Ollama**       | GGUF, 단순성              | **로컬 개발**, 데스크톱, 엣지 환경에서의 실험   | 사용법이 매우 쉽지만, 고성능/다중 사용자 서빙용은 아님.                             |
| **TensorRT-LLM** | NVIDIA 커널 최적화        | **최저 지연 시간**이 중요한 특정 모델 서빙      | NVIDIA GPU에서 최고의 성능을 내지만, 모델별 컴파일 과정이 복잡하고 유연성이 떨어짐. |

______________________________________________________________________

## 5. 예상 면접 질문 및 모범 답안

### Q1. vLLM과 TGI 중 어떤 서빙 프레임워크를 선택해야 할까요?

**A.** **최대 처리량(Throughput)** 이 최우선 목표라면 vLLM을, Hugging Face 생태계와의 통합 및 **안정적인 프로덕션 기능**이 더 중요하다면 TGI를 선택하는 것이 일반적입니다. vLLM은 PagedAttention을 통한 효율적인 메모리 관리로 동시 사용자 처리 능력에 강점이 있고, TGI는 텐서 병렬화 지원과 안정성 면에서 강점이 있습니다.

**\[추가 설명\]**

- **vLLM 선택**: 서비스의 사용자가 많고, 입출력 길이가 매우 가변적인 워크로드에서 비용 효율을 극대화하고 싶을 때 유리합니다.
- **TGI 선택**: 이미 Hugging Face 생태계를 많이 사용하고 있거나, 멀티 GPU 서빙을 위한 텐서 병렬화 기능이 중요하며, 프로메테우스 메트릭 등 검증된 운영 기능이 필요할 때 더 나은 선택일 수 있습니다.
- **최신 동향**: 두 프레임워크 모두 빠르게 발전하며 서로의 장점을 흡수하고 있어, 실제 배포 전 두 프레임워크로 성능 벤치마크를 직접 수행해보는 것이 가장 좋습니다.

### Q2. 연속 배칭(Continuous Batching)과 추측 디코딩(Speculative Decoding)의 이점과 한계는 무엇인가요?

**A.** **연속 배칭**은 GPU의 유휴 시간을 줄여 **전체 처리량(Throughput)을 극대화**하는 기술이며, **추측 디코딩**은 여러 토큰을 한 번에 검증하여 **개별 요청의 지연 시간(Latency)을 줄이는** 기술입니다. 연속 배칭의 한계는 스케줄링 로직의 복잡성이고, 추측 디코딩의 한계는 초안 모델의 예측이 자주 틀리면 오히려 성능이 저하될 수 있다는 점입니다.

**\[추가 설명\]**

- **연속 배칭**: GPU를 '공장'에 비유하면, 정적 배칭은 모든 작업자가 일을 끝낼 때까지 다음 주문을 받지 않는 것이고, 연속 배칭은 한 작업자가 일을 끝내자마자 그 자리에 바로 다음 주문을 투입하는 것과 같습니다. GPU 활용률을 크게 높입니다.
- **추측 디코딩**: '상사(메인 모델)'가 한 글자씩 결재하는 대신, '부하직원(초안 모델)'이 보고서 초안(여러 토큰)을 작성해오면 한 번에 검토하고 승인하는 것과 같습니다. 부하직원이 일을 잘하면 결재 속도가 매우 빨라지지만, 일을 못하면 다시 작성시켜야 하므로 시간이 더 걸릴 수 있습니다.

### Q3. 양자화로 인해 모델의 정확도가 하락할 때, 어떤 보정 전략을 사용할 수 있나요?

**A.** 가장 먼저, **GPTQ나 AWQ처럼 정확도 손실을 최소화하는 고급 양자화 기법**을 적용해야 합니다. 만약 그래도 정확도가 부족하다면, 모델의 모든 레이어를 양자화하는 대신 **중요한 레이어(예: 임베딩, 최종 LM 헤드)는 원래 정밀도(FP16 등)로 남겨두고, 덜 민감한 중간 레이어만 양자화**하는 '혼합 정밀도(Mixed-Precision)' 전략을 사용할 수 있습니다.

**\[추가 설명\]**

- **고급 양자화 기법**: 단순한 양자화 대신, GPTQ는 보정용 데이터를 사용하여 양자화 오차를 보정하고, AWQ는 모델 성능에 중요한 '핵심 가중치'를 보호하며 양자화를 수행하여 정확도 하락을 최소화합니다.
- **혼합 정밀도**: 모델의 모든 부분이 양자화에 동일하게 민감하지 않다는 점을 이용합니다. 일반적으로 모델의 처음과 끝 레이어가 정확도에 가장 큰 영향을 미치므로, 이 부분은 고정밀도로 유지하고 거대한 중간 트랜스포머 블록들만 저정밀도로 양자화하여, 메모리 이점과 정확도 사이의 균형을 맞춥니다.
- **QAT (Quantization-Aware Training)**: 가장 강력한 방법으로, 파인튜닝 과정 자체에 양자화 오차를 포함시켜 모델이 양자화에 '적응'하도록 학습시킵니다. 비용은 가장 높지만, 주어진 비트 수에서 최고의 정확도를 얻을 수 있습니다.

______________________________________________________________________

## 5. See also

- [LLM 아키텍처 및 어텐션 최적화](./llm-architecture.md)
- [파인튜닝 및 적응 (QLoRA)](./fine-tuning-and-adaptation.md)
- [운영 및 평가 게이팅](../5-6-agentops-%EC%9A%B4%EC%98%81-and-%EC%9E%90%EB%8F%99%ED%99%94/evaluation-monitoring-ops.md)
