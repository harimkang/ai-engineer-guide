---
title: "LLM 아키텍처 핵심: Transformer, Attention, KV 캐시"
date: "2025-10-29"
tags: ["Agentic AI", "LLM", "Architecture"]
difficulty: "medium"
---

# LLM 아키텍처 핵심

## 1. 핵심 개념 (Core Concept)

에이전트 맥락에서 필요한 Transformer/Attention, KV 캐시의 요점을 요약하고 4장(LLM) 상세 문서로 연결함.

---

## 2. 상세 설명 (Detailed Explanation)

### 2.1 Decoder-only 구조와 RPE
### 2.2 희소 어텐션·MoE 개요
### 2.3 KV 캐시와 지연 최적화

---

## 3. 예시 (Example)

- KV 캐시 공유로 호출 비용 절감 개요.

---

## 4. 예상 면접 질문 (Potential Interview Questions)

- MoE의 장단점과 라우팅 이슈는?

---

## 5. 더 읽어보기 (Further Reading)

- 4장 LLM 관련 문서들(서빙/양자화/컨텍스트 확장 등)

