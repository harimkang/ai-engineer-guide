---
title: "LLM 아키텍처 핵심: Transformer, Attention, KV 캐시"
date: "2025-10-29"
tags: ["Agentic AI", "LLM", "Architecture", "Transformer", "Attention"]
difficulty: "hard"
---

# LLM 아키텍처 핵심

## 1. 핵심 개념 (Core Concept)

현대 LLM의 근간은 **트랜스포머(Transformer)**, 그 중에서도 **디코더-온리(Decoder-only)** 아키텍처입니다. 이 아키텍처의 심장은 **셀프 어텐션(Self-Attention)** 메커니즘으로, 문장 내 단어들이 서로에게 얼마나 '주목'해야 할지를 계산하여 문맥을 파악합니다. 추론 시에는 이 어텐션 계산의 중간 결과물을 저장하는 **KV 캐시**가 성능의 핵심 병목이 되며, 따라서 대부분의 추론 최적화는 어텐션 계산을 효율화하고 KV 캐시 메모리를 절약하는 데 집중됩니다.

*Note: 아래 다이어그램을 위한 이미지를 `transformer-decoder-block.png` 에 추가해주세요.*
![Transformer Decoder Block Diagram](../../images/transformer-decoder-block.png)

---

## 2. 상세 설명 (Detailed Explanation)

### 2.1 셀프 어텐션 메커니즘 (Query, Key, Value)

어텐션은 문장 속 한 단어가 다른 모든 단어와의 관계를 파악하는 과정입니다. 이는 세 가지 요소로 이루어집니다.

- **Query (Q)**: 현재 내가 주목하고 있는 단어. (예: "it")
- **Key (K)**: 다른 단어들이 자신을 설명하기 위해 들고 있는 '간판'. (예: "The cat", "The mat")
- **Value (V)**: 다른 단어들이 실제로 담고 있는 '의미'.

**동작 방식 (비유)**: `"The cat sat on the mat because it was tired."` 라는 문장에서 `it` 이라는 단어(Query)가 자신의 의미를 명확히 하고자 합니다. `it`은 다른 모든 단어들에게 자신의 '질문(Query)'을 던집니다. 다른 단어들은 자신의 '간판(Key)'을 보여주고, `it`은 자신의 질문과 가장 관련 높은 간판(Key), 즉 `The cat`을 찾아냅니다. 그리고 그 간판을 들고 있던 단어의 실제 '의미(Value)'를 가져와 자신의 의미에 크게 반영합니다. 이 과정을 통해 `it`이 `mat`이 아닌 `cat`을 가리킨다는 것을 학습하게 됩니다.

### 2.2 어텐션의 변형: MHA, MQA, GQA

하나의 어텐션 헤드가 한 종류의 관계만 본다면, 여러 개의 헤드를 두어(Multi-Head Attention) 다양한 종류의 관계(예: 문법적 관계, 의미적 관계)를 동시에 보도록 한 것이 MHA입니다. 하지만 헤드가 많아질수록 KV 캐시 메모리도 비례하여 커지는 문제가 발생하여, 이를 해결하기 위한 변형들이 등장했습니다.

| 종류 | Key/Value 헤드 구조 | 장점 | 단점 |
| :--- | :--- | :--- | :--- |
| **MHA (다중 헤드)** | 모든 Query 헤드가 자신만의 Key/Value 헤드를 가짐 | 최고 성능, 가장 풍부한 표현력 | 가장 많은 KV 캐시 메모리 사용 |
| **GQA (그룹 쿼리)** | 여러 Query 헤드 그룹이 하나의 Key/Value 헤드를 공유 | 성능과 메모리 사이의 좋은 균형 | MHA보다 약간의 성능 저하 |
| **MQA (다중 쿼리)** | 모든 Query 헤드가 단 하나의 Key/Value 헤드를 공유 | 최소의 KV 캐시 메모리 사용 | 가장 큰 성능 저하 가능성 |

> **실무 가이드**: GQA는 MQA만큼 메모리를 절약하면서도 MHA만큼의 성능을 내는 경우가 많아, 최근 고성능 모델(예: Llama 3)에서 표준처럼 채택되고 있습니다.

### 2.3 KV 캐시: 추론 속도의 핵심

- **원리**: LLM은 다음 토큰을 예측할 때, 이전에 나왔던 모든 토큰들의 Key와 Value를 다시 계산할 필요가 없습니다. 한 번 계산된 Key와 Value 값을 GPU 메모리(KV 캐시)에 저장해두고, 새로운 토큰이 생성될 때마다 이 캐시를 재사용합니다. 이것이 없다면, 토큰을 하나 생성할 때마다 전체 문장을 처음부터 다시 읽는 것과 같아 매우 비효율적입니다.
- **병목**: KV 캐시는 `(배치 크기 × 시퀀스 길이 × 헤드 수 × ...)`에 비례하여 커지므로, 긴 문맥과 많은 동시 사용자를 처리할 때 GPU 메모리를 가장 많이 차지하는 주범이 됩니다.

### 2.4 MoE (Mixture-of-Experts)

- **원리**: 모델 내부에 여러 개의 '전문가(Expert, 피드포워드 네트워크)'를 두고, 각 토큰마다 '라우터(Router)'가 가장 적합한 전문가 몇 명에게만 작업을 할당하는 방식입니다.
- **장점**: 모델의 전체 파라미터 수를 크게 늘려 용량(knowledge)을 키우면서도, 추론 시에는 일부 파라미터만 활성화되므로 계산 비용(FLOPs)은 작게 유지할 수 있습니다.
- **단점**: 일부 전문가에게만 작업이 몰리는 '라우팅 불균형' 문제, 그리고 여러 전문가를 메모리에 올려둬야 하는 서빙의 복잡성이 있습니다.

---

## 3. 예상 면접 질문 및 모범 답안

### Q1. MoE(Mixture-of-Experts)의 장단점과 주요 라우팅 이슈는 무엇인가요?

**A.** MoE의 가장 큰 **장점**은, 추론 비용(FLOPs)은 거의 늘리지 않으면서 모델의 파라미터 수를 극적으로 늘려 **모델의 용량과 성능을 향상**시킬 수 있다는 점입니다. **단점**은 일부 전문가에게만 작업이 쏠리는 **라우팅 불균형(Load Imbalance)** 문제와, 거대한 모델을 서빙해야 하는 **운영의 복잡성**입니다.

**[추가 설명]**
- **장점 (효율적인 확장)**: 8명의 전문가가 있는 MoE 모델은, 추론 시 각 토큰마다 2명의 전문가만 활성화시킵니다. 따라서 파라미터 수는 8배 크지만, 실제 계산량은 2/8, 즉 1/4 크기의 일반 모델과 비슷하여 매우 효율적입니다.
- **라우팅 이슈**: '라우터' 네트워크가 특정 토큰을 어떤 전문가에게 보낼지 결정하는데, 이 라우터가 잘못 학습되면 항상 1, 2번 전문가에게만 모든 토큰을 보내고 나머지 전문가들은 노는 '부익부 빈익빈' 현상이 발생합니다. 이를 해결하기 위해 학습 시 전문가들에게 작업을 골고루 분배하도록 유도하는 '보조 손실 함수(Auxiliary Loss)'를 사용합니다.

### Q2. GQA/MQA가 KV 캐시 메모리에 어떤 이점을 주나요?

**A.** GQA와 MQA는 여러 개의 Query 헤드가 **하나의 Key/Value 헤드를 공유**하도록 하여, **KV 캐시의 크기를 획기적으로 줄여주는** 이점을 제공합니다. KV 캐시는 추론 시 메모리 병목의 주된 원인이므로, 이를 줄이면 같은 GPU 메모리로 더 긴 컨텍스트나 더 많은 동시 사용자를 처리할 수 있습니다.

**[추가 설명]**
- **문제점**: 일반적인 MHA(Multi-Head Attention)에서는 32개의 Query 헤드가 있다면, 32세트의 Key, Value 캐시가 필요합니다.
- **해결책 (공유)**:
  - **MQA**: 32개의 Query 헤드가 단 1세트의 K/V 캐시만 공유합니다. KV 캐시 크기가 1/32로 줄어듭니다.
  - **GQA**: 8개의 Query 헤드가 1세트의 K/V 캐시를 공유한다면(그룹 크기=8), 총 4세트의 K/V 캐시만 필요합니다. KV 캐시 크기가 1/8로 줄어듭니다.
- **효과**: 메모리 사용량이 줄어들 뿐만 아니라, 디코딩 단계에서 GPU가 읽어야 할 데이터의 양도 줄어들어 추론 속도(지연 시간) 또한 개선되는 효과가 있습니다. GQA는 MQA의 메모리 절약 효과를 상당 부분 유지하면서 MHA의 성능을 거의 잃지 않아, 최근 가장 각광받는 아키텍처입니다.

### Q3. FlashAttention과 일반 어텐션의 차이점과 그 한계는 무엇인가요?

**A.** FlashAttention은 새로운 알고리즘이 아니라, **기존 어텐션의 연산 과정을 하드웨어(GPU)에 맞게 극도로 최적화한 구현체**입니다. 핵심 차이는 거대한 중간 결과물인 '어텐션 행렬'을 GPU의 느린 주 메모리(HBM)에 쓰는 과정을 없애고, 훨씬 빠른 온칩 메모리(SRAM) 내에서 모든 계산을 끝내는 것입니다. 이를 통해 **속도와 메모리 효율을 크게 향상**시킵니다. 한계는 **최신 GPU 하드웨어에 의존적**이라는 점입니다.

**[추가 설명]**
- **일반 어텐션의 병목**: `Softmax(Q * K^T)`를 계산할 때, 시퀀스 길이가 N이면 N x N 크기의 거대한 행렬이 만들어집니다. 이 행렬을 GPU의 주 메모리(HBM)에 썼다가 다시 읽어오는 과정은, 실제 행렬 곱셈 연산보다 훨씬 느린 메모리 I/O 병목을 유발합니다.
- **FlashAttention의 해결책 (Tiling & Kernel Fusion)**: 전체 행렬을 한 번에 계산하지 않고, 작은 '타일(Tile)' 단위로 쪼개어 계산합니다. 각 타일 계산은 GPU의 매우 빠른 SRAM 안에서 모두 이루어지며, 중간 결과를 HBM에 쓰지 않습니다. 이 '커널 퓨전' 기법 덕분에 메모리 I/O 병목이 사라집니다.
- **한계**: FlashAttention은 알고리즘적 개선이 아닌, CUDA 커널 수준의 저수준 최적화이므로, 그 효과를 온전히 누리기 위해서는 NVIDIA의 최신 GPU(Ampere 아키텍처 이상)가 필요합니다. 또한, 수학적으로는 일반 어텐션과 동일한 결과를 내므로, 모델의 '품질'을 바꾸는 것이 아니라 '성능'을 개선하는 기술이라는 점을 이해해야 합니다.

---

## 5. See also

- [추론 최적화 및 서빙](./inference-optimization-and-serving.md)
- [파인튜닝 및 적응](./fine-tuning-and-adaptation.md)