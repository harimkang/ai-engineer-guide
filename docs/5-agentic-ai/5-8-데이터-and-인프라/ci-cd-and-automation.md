---
title: "CI/CD & 자동화: 평가 파이프라인, 재현성, 게이팅"
date: "2025-10-29"
tags: ["Agentic AI", "LLMOps", "CI/CD", "Automation", "GitHub Actions"]
difficulty: "hard"
---

# CI/CD & 자동화

## 1. 핵심 개념 (Core Concept)

LLM 시스템을 위한 CI/CD는 기존 소프트웨어의 CI/CD와 근본적으로 다릅니다. 코드의 논리적 정확성을 검증하는 유닛 테스트만으로는 부족하며, 프롬프트나 모델의 작은 변화가 가져올 예측 불가능한 품질 변화를 검증하기 위해 **자동화된 평가 파이프라인을 CI/CD의 핵심**으로 삼아야 합니다. 즉, '테스트'가 '평가'로 대체되며, 이 평가 결과를 바탕으로 배포 여부를 결정하는 **게이팅(Gating)** 과 점진적 배포 자동화가 핵심입니다.

*Note: 아래 다이어그램을 위한 이미지를 `docs/images/agent-cicd-pipeline.png` 에 추가해주세요.*
![Agent CI/CD Pipeline Diagram](../../images/agent-cicd-pipeline.png)

---

## 2. 에이전트를 위한 CI/CD 파이프라인 단계별 가이드

### 2.1 1단계 (CI): Pull Request 생성 시 자동 회귀 평가

1.  **트리거**: 개발자가 새 프롬프트나 변경된 코드를 담은 Pull Request(PR)를 생성합니다.
2.  **실행**: GitHub Actions 같은 CI 도구가 이를 감지하고, 사전에 정의된 '골든 데이터셋'(100~500개의 대표 질문/답변 쌍)에 대해 평가 스크립트를 실행합니다.
3.  **비교 및 리포팅**: 스크립트는 변경된 버전과 기존 `main` 브랜치 버전의 성능을 모두 실행하여, 주요 지표(예: Judge 점수, 지연 시간, 비용)의 변화를 비교하는 리포트를 생성합니다.
4.  **결과 게시**: 생성된 비교 리포트를 PR에 코멘트로 게시하여, 리뷰어가 변경의 영향을 한눈에 파악할 수 있도록 합니다.
5.  **게이팅 (Gating)**: '핵심 품질 지표 5% 이상 하락' 또는 '비용 10% 이상 증가' 등 사전에 정의된 기준을 위반하면, CI 빌드를 실패 처리하여 문제가 있는 코드가 병합되는 것을 원천적으로 차단합니다.

### 2.2 2단계 (CD): Main 브랜치 병합 시 자동 배포

1.  **트리거**: CI를 통과한 PR이 `main` 브랜치에 병합됩니다.
2.  **아티팩트 빌드**: Docker 이미지, 프롬프트 버전 파일 등 배포에 필요한 아티팩트를 빌드하고 버전 태그를 붙여 레지스트리에 푸시합니다.
3.  **점진적 배포 시작**: `agent-lifecycle-ops.md`에서 설명한 섀도우, 카나리, 점진적 롤아웃 등의 자동화된 배포 파이프라인을 실행합니다. 각 단계는 자체적인 통과 기준(SLO)을 가지며, 기준 미달 시 자동으로 롤백됩니다.

### 2.3 스케줄링된 자동화 작업 (Scheduled Jobs)

- **야간 배치 평가**: CI에서 사용하는 작은 데이터셋보다 훨씬 큰 수만 건의 데이터셋으로 야간에 평가를 실행하여, 더 정확한 통계적 성능을 추적합니다.
- **야간 RAG 인덱싱**: 매일 추가되는 새로운 문서를 RAG 시스템의 벡터 DB에 자동으로 임베딩하고 인덱싱합니다.
- **정기 모델 파인튜닝**: 수집된 사용자 피드백이나 새로운 데이터를 바탕으로, 정기적으로(예: 매주) 모델을 자동으로 파인튜닝하고, 그 결과를 CI 파이프라인에 제출하여 성능을 검증합니다.

---

## 3. 예상 면접 질문 및 모범 답안

### Q1. 프롬프트 회귀 테스트를 CI 파이프라인에 어떻게 통합하나요?

**A.** GitHub Actions와 같은 CI 도구를 사용하여, **Pull Request가 생성될 때마다 평가 스크립트를 자동으로 실행**하도록 구성합니다. 이 스크립트는 변경된 프롬프트와 기존 프롬프트의 성능을 '골든 데이터셋' 상에서 비교하고, 그 결과를 PR 코멘트로 게시하며, **핵심 지표가 악화되면 빌드를 실패시켜 병합을 막는 방식**으로 통합합니다.

**[추가 설명]**
1.  **골든 데이터셋**: 일반적인 케이스와 엣지 케이스를 포함하는 100~500개 규모의 고품질 테스트셋을 Git 레포지토리에 포함시킵니다.
2.  **평가 스크립트**: 이 스크립트는 PR의 프롬프트와 `main` 브랜치의 프롬프트를 각각 실행하여, Judge 점수, F1, 지연 시간 등의 지표를 계산하고 그 차이를 JSON이나 마크다운 리포트로 생성합니다.
3.  **CI 설정 (예: GitHub Actions)**: PR이 열릴 때마다 이 스크립트를 실행하도록 `on: [pull_request]` 워크플로우를 설정합니다.
4.  **게이팅**: 스크립트 마지막에 `if new_score < old_score * 0.95: exit(1)` 와 같이, 점수가 5% 이상 하락하면 CI 프로세스를 실패시키는 '게이트'를 구현합니다. 이를 통해 품질 저하를 초래하는 변경이 리뷰 단계에서부터 걸러지게 됩니다.

### Q2. 게이팅(Gating) 기준은 어떻게 정하고, 운영에 어떻게 반영하나요?

**A.** 게이팅 기준은 **품질, 비용, 속도**에 대한 서비스 수준 목표(SLO)를 기반으로 정합니다. 예를 들어, `Judge 점수 >= 4.0`, `요청당 비용 <= $0.01` 와 같이 설정합니다. 이 기준은 **CI 단계에서는 코드 병합을 막는 기준**으로, **운영(프로덕션) 중에는 경보(Alert)를 울리거나 자동 롤백을 트리거하는 기준**으로 동일하게 반영됩니다.

**[추가 설명]**
- **기준 설정**: 먼저 현재 안정적으로 운영 중인 버전의 성능을 측정하여 '베이스라인'을 설정합니다. 그 후, 비즈니스 요구사항에 따라 "이것 이하로 떨어지면 안 된다"는 최소 기준선, 즉 SLO를 정의합니다.
- **운영 반영**:
  - **CI 게이팅**: CI 스크립트가 평가 후 이 SLO를 충족하는지 검사하여, 충족하지 못하면 빌드를 실패시킵니다.
  - **CD 게이팅**: 카나리 배포 시, 새 버전의 성능이 SLO를 위반하면 배포를 자동으로 중단하고 롤백합니다.
  - **프로덕션 경보**: 운영 모니터링 시스템이 실시간으로 SLO를 감시하다가, 5분 이상 위반 상태가 지속되면 담당자에게 경보를 보냅니다.

### Q3. 야간에 실행되는 대규모 배치 평가나 RAG 인덱싱 작업이 실패했을 때의 복구 전략은 무엇인가요?

**A.** **배치 평가** 실패 시에는 이전의 유효한 평가 결과를 기준으로 삼고 배포를 보수적으로 막되, 서비스 자체에는 영향이 없으므로 팀에 알림만 보냅니다. **RAG 인덱싱** 실패 시에는 **블루-그린 배포 전략**을 사용하여, 서비스는 이전의 안정적인 인덱스를 계속 사용하도록 하고, 실패한 새 인덱스는 폐기하여 사용자 영향이 전혀 없도록 합니다.

**[추가 설명]**
- **배치 평가 실패**: 오프라인 작업이므로 사용자에게 직접적인 영향은 없습니다. 실패 시 즉시 팀에 알림을 보내 원인을 디버깅하고 수동으로 재실행하도록 합니다. 이 동안 CI/CD 파이프라인은 '가장 최근의 성공한 평가 결과'를 기준으로 작동하도록 설정합니다.
- **RAG 인덱싱 실패 (블루-그린 전략)**:
  1.  운영 중인 인덱스(블루)는 절대 직접 수정하지 않습니다.
  2.  인덱싱 작업은 항상 완전히 새로운 인덱스(그린)를 만드는 것을 목표로 합니다.
  3.  작업이 성공적으로 끝나고 새 인덱스에 대한 간단한 검증까지 통과했을 때만, 애플리케이션이 바라보는 인덱스 경로를 블루에서 그린으로 '원자적(atomic)으로' 전환합니다.
  4.  만약 인덱싱 작업이 중간에 실패하면, 불완전한 그린 인덱스는 그냥 버리면 됩니다. 서비스는 아무런 영향 없이 계속해서 안정적인 블루 인덱스를 사용하고 있게 됩니다.

---

## 4. See also

- [에이전트 라이프사이클 운영](../5-6-agentops-운영-and-자동화/agent-lifecycle-ops.md)
- [평가 및 벤치마크](../5-5-프롬프트-엔지니어링-and-평가/prompt-evaluation-and-benchmarks.md)