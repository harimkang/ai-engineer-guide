---
title: "XAI 주요 기법: SHAP, LIME, 그리고 반사실적 설명"
date: "2025-07-08"
tags: ["책임 AI", "해석가능성", "XAI", "SHAP", "LIME", "Counterfactuals"]
difficulty: "hard"
---

# XAI 주요 기법: SHAP, LIME, 그리고 반사실적 설명

## 1. 핵심 개념 (Core Concept)

**SHAP**, **LIME**, **반사실적 설명(Counterfactual Explanations)**은 복잡한 "블랙박스" AI 모델의 예측 결과를 해석하기 위한 대표적인 XAI(eXplainable AI) 기법들입니다. **LIME**은 특정 예측 주변을 탐색하여 지역적으로 단순한 모델을 만들어 "왜 이 예측이 나왔는가?"를 설명합니다. **SHAP**은 게임 이론에 기반하여 각 특징(feature)이 예측에 얼마나 기여했는지를 정확히 측정하여 지역적 설명과 전역적 설명을 모두 제공합니다. **반사실적 설명**은 "결과를 바꾸려면 무엇을 해야 하는가?"라는 질문에 답하며 사용자에게 실행 가능한(actionable) 대안을 제시하는 데 중점을 둡니다.

---

## 2. 상세 설명 (Detailed Explanation)

이 세 가지 기법은 모델을 해석하는 관점과 방식에서 차이가 있으며, 상호 보완적인 역할을 합니다.

### 2.1 LIME (Local Interpretable Model-agnostic Explanations)

LIME은 이름 그대로 **지역적(Local)**인 해석을 제공하는 **모델 불특정적(Model-agnostic)** 기법입니다. 복잡한 모델의 전체를 이해하려 하기보다, **하나의 예측**에 집중합니다.

*   **작동 원리**:
    1.  설명하려는 예측 데이터 포인트 주변에 다수의 가상 데이터를 생성합니다.
    2.  생성된 가상 데이터들을 원래의 블랙박스 모델에 입력하여 예측값을 얻습니다.
    3.  원본 데이터에 가까운 가상 데이터에 더 높은 가중치를 부여하여, 이 데이터들을 가장 잘 설명하는 단순하고 해석 가능한 모델(예: 선형 회귀, 의사결정 트리)을 학습시킵니다.
    4.  이 단순한 **대리 모델(Surrogate Model)**의 계수(coefficients)나 규칙을 통해 원본 모델의 예측 이유를 설명합니다.

*   **장점**: 계산이 비교적 빠르고, 결과가 직관적이어서 이해하기 쉽습니다.
*   **단점**: 주변 데이터를 어떻게 샘플링하느냐에 따라 설명 결과가 달라질 수 있어 **불안정성**이 존재하며, 설명이 해당 예측에만 국한됩니다.

### 2.2 SHAP (SHapley Additive exPlanations)

SHAP은 협력 게임 이론의 **섀플리 값(Shapley Value)**을 활용하여, 각 특징이 최종 예측에 기여한 정도를 엄밀하게 계산합니다. 이는 LIME보다 이론적으로 더 견고하며, 지역적 설명과 전역적 설명을 모두 제공합니다.

*   **작동 원리**: 어떤 특징의 기여도를 계산하기 위해, 해당 특징을 포함하는 모든 가능한 특징 조합에 대해 예측값의 변화를 측정하고 평균을 냅니다. 이를 통해 각 특징의 정확한 기여분(SHAP 값)을 계산합니다.
    *   **긍정적 SHAP 값**: 해당 특징이 예측 확률을 높이는 방향으로 작용했음을 의미합니다.
    *   **부정적 SHAP 값**: 해당 특징이 예측 확률을 낮추는 방향으로 작용했음을 의미합니다.

*   **장점**: 이론적 기반이 탄탄하여 결과가 **일관성** 있고 신뢰도가 높습니다. 특징 간의 상호작용 효과까지 고려할 수 있으며, 여러 예측에 대한 SHAP 값을 종합하여 모델 전체를 설명하는 **전역적 설명**이 가능합니다.
*   **단점**: 모든 특징 조합을 고려해야 하므로 특징의 수가 많아지면 **계산 비용이 매우 높아**질 수 있습니다. (이를 해결하기 위해 KernelSHAP, TreeSHAP 등 최적화된 알고리즘이 사용됩니다.)

### 2.3 반사실적 설명 (Counterfactual Explanations)

반사실적 설명은 "만약 ...했다면 어땠을까?"라는 질문에 답하는 방식으로 설명을 제공합니다. 즉, 모델의 예측 결과를 바꾸기 위해 필요한 **최소한의 입력 특징 변화**를 찾아 제시합니다.

*   **작동 원리**: "대출 신청이 거절(예측 결과)된 고객이, 만약 연 소득이 500만원 더 높고 부채가 1,000만원 더 적었다면 대출이 승인(원하는 결과)되었을 것이다"와 같은 형태의 설명을 생성합니다. 이를 위해 원하는 예측 결과를 내는 가장 가까운 데이터 포인트를 탐색하는 최적화 문제를 풉니다.

*   **장점**: 사용자에게 **실행 가능한(actionable) 조언**을 제공할 수 있어 매우 실용적입니다. 예측 결과를 바꾸는 핵심 요인을 알려주므로 인과관계에 대한 통찰을 줍니다.
*   **단점**: 현실적으로 변경 불가능한 특징(예: 나이, 인종)에 대한 대안을 제시할 수 있으며, 유의미한 반사실적 설명을 찾는 데 계산 비용이 많이 들 수 있습니다.

### 2.4 비교 요약

| 구분 | LIME | SHAP | Counterfactuals |
| :--- | :--- | :--- | :--- |
| **핵심 질문** | "이 예측은 **왜** 이렇게 나왔는가?" | "각 특징이 예측에 **얼마나 기여**했는가?" | "결과를 바꾸려면 **무엇을** 해야 하는가?" |
| **설명 범위** | 지역적 (Local) | **지역적 & 전역적 (Global)** | 지역적 (Local) |
| **이론적 기반** | 지역적 대리 모델 | **게임 이론 (섀플리 값)** | 최적화 문제 |
| **강점** | 속도, 직관성 | **신뢰성, 일관성, 정확성** | **실행 가능성 (Actionable)** |
| **약점** | 불안정성 | 계산 복잡성 | 다중 해(Multiple solutions) 가능성 |

---

## 3. 예시 (Example)

### 시나리오: 신용카드 발급 심사 AI

한 고객의 신용카드 발급 신청이 AI에 의해 거절되었습니다. 이 결과를 각 기법으로 설명해봅시다.

*   **LIME**: "고객님의 신청은 **연체 기록**과 **낮은 연 소득**이라는 특징 때문에 거절된 것으로 보입니다."
    *   *해석: 예측 시점에 가장 큰 영향을 준 지역적 특징들을 보여줌.*

*   **SHAP**: "고객님의 예측 점수는 기본 점수에서 시작하여, **연체 기록** 때문에 30점 감점, **낮은 연 소득** 때문에 20점 감점되었지만, **오랜 신용 거래 기간** 덕분에 10점 가산되어 최종적으로 거절 기준을 넘지 못했습니다."
    *   *해석: 각 특징의 긍정/부정 기여도를 수치로 정확하게 보여줌.*

*   **반사실적 설명**: "만약 고객님의 **연 소득이 현재보다 1,000만원 더 높았다면**, 다른 조건이 동일하더라도 발급이 승인되었을 것입니다."
    *   *해석: 고객이 결과를 바꾸기 위해 시도해볼 수 있는 구체적인 행동을 제시함.*

---

## 4. 예상 면접 질문 (Potential Interview Questions)

*   **Q. LIME과 SHAP 중 어떤 것을 선호하며, 그 이유는 무엇인가요?**
    *   **A.** 상황에 따라 다릅니다. **빠르고 직관적인 해석**이 필요하고 개별 예측에 대한 대략적인 이유만 파악해도 충분하다면 LIME을 사용하겠습니다. 하지만, 금융이나 의료 분야처럼 **설명의 신뢰성과 정확성**이 매우 중요하고, 모델의 전반적인 동작까지 이해해야 하는 상황이라면 계산 비용이 더 들더라도 이론적 보장이 있는 SHAP을 선호할 것입니다. SHAP은 일관성이 보장되고 전역적 설명까지 가능하여 모델 디버깅 및 감사에 더 적합하기 때문입니다.

*   **Q. 반사실적 설명이 다른 설명 기법들과 차별화되는 가장 큰 장점은 무엇인가요?**
    *   **A.** **실행 가능성(Actionability)**입니다. LIME이나 SHAP은 예측이 '왜' 그렇게 나왔는지 과거의 원인을 설명하는 데 중점을 두지만, 반사실적 설명은 사용자가 원하는 결과를 얻기 위해 '무엇을' 해야 하는지 미래의 행동 지침을 제공합니다. 이는 사용자에게 직접적인 도움을 주고, AI의 결정에 대해 이의를 제기하고 개선할 수 있는 구체적인 방법을 제시한다는 점에서 매우 강력한 장점을 가집니다.

*   **Q. SHAP의 계산 비용이 비싼 이유는 무엇이며, 이를 어떻게 해결할 수 있나요?**
    *   **A.** SHAP은 특정 특징의 기여도를 계산하기 위해 해당 특징을 제외한 모든 가능한 특징들의 조합(subset)에 대해 모델 예측을 반복적으로 수행해야 하기 때문에 계산적으로 매우 비쌉니다. 특징이 N개일 때, 2^N개의 조합이 가능하기 때문입니다. 이를 해결하기 위해 실제로는 모든 조합을 계산하는 대신, **KernelSHAP**과 같이 LIME의 아이디어를 차용하여 샘플링 기반으로 근사치를 계산하거나, **TreeSHAP**처럼 트리 기반 모델(예: XGBoost, LightGBM)에 최적화된 알고리즘을 사용하여 다항 시간 내에 정확한 섀플리 값을 계산하는 방법을 사용합니다.

---

## 5. 더 읽어보기 (Further Reading)

*   [SHAP (SHapley Additive exPlanations) GitHub](https://github.com/slundberg/shap)
*   [LIME (Local Interpretable Model-agnostic Explanations) GitHub](https://github.com/marcotcr/lime)
*   [Interpretable Machine Learning by Christoph Molnar - Chapter on Counterfactual Explanations](https://christophm.github.io/interpretable-ml-book/counterfactual.html)
*   [A Unified Approach to Interpreting Model Predictions (SHAP Paper)](https://arxiv.org/abs/1705.07874)
*   ["Why Should I Trust You?": Explaining the Predictions of Any Classifier (LIME Paper)](https://arxiv.org/abs/1602.04938)