---
title: "배포 고려사항 (서버리스, 컨테이너, GPU)"
date: "2025-07-06"
tags: ["Agentic AI", "Playbook", "Deployment", "Serverless", "GPU"]
difficulty: "medium"
---

# 배포 고려사항 (서버리스, 컨테이너, GPU)

## 1. 핵심 개념 (Core Concept)

AI 에이전트를 프로덕션 환경에 배포하는 것은 단순히 모델을 API로 제공하는 것을 넘어, 에이전트의 특성(상태 유지, 장기 실행, 다양한 툴 호출)에 맞는 인프라를 선택하는 전략적인 과정임. 워크로드의 종류에 따라 **서버리스(Serverless)** 아키텍처로 비용 효율성을 극대화하거나, **컨테이너(Container)** 기반으로 일관되고 안정적인 환경을 제공할 수 있음. 또한, LLM 추론 성능을 좌우하는 **GPU** 자원을 어떻게 효율적으로 할당하고 관리할 것인지 결정해야 함.

---

## 2. 상세 설명 (Detailed Explanation)

### 2.1 워크로드 유형별 배포 전략

| 워크로드 유형 | 특징 | 추천 아키텍처 | 주요 고려사항 |
| :--- | :--- | :--- | :--- |
| **실시간 대화형 에이전트** | 낮은 지연 시간(Low Latency)이 매우 중요. 항상 준비 상태여야 함. | **컨테이너 (Docker/Kubernetes)** + **전용 GPU** | - **응답성**: 항상 켜져 있는 컨테이너로 콜드 스타트 방지.<br>- **안정성**: 예측 가능한 트래픽에 대해 안정적인 성능 제공. |
| **비동기·배치 작업 에이전트** | 간헐적, 예측 불가능한 트래픽. 실행 시간이 길 수 있음. | **서버리스 (AWS Lambda, Google Cloud Functions)** | - **비용 효율성**: 유휴 시간에는 비용이 발생하지 않음 (Scale-to-zero).<br>- **콜드 스타트**: 첫 요청 시 지연이 발생할 수 있으나, 비동기 작업에는 영향이 적음. |
| **복잡한 멀티-에이전트 시스템** | 여러 에이전트가 상호작용. 상태 관리가 중요. | **컨테이너 오케스트레이션 (Kubernetes, LangGraph)** | - **상태 관리**: 각 에이전트의 상태를 안정적으로 유지하고 공유해야 함.<br>- **네트워킹**: 에이전트 간의 통신이 원활해야 함. |

### 2.2 서버리스 GPU: 비용과 성능의 트레이드오프

서버리스 플랫폼에서 GPU를 사용하는 것은 간헐적인 추론 작업에 매우 비용 효율적일 수 있음. 하지만 **콜드 스타트(Cold Start)**라는 치명적인 단점을 고려해야 함.

*   **콜드 스타트**: 요청이 없을 때 0으로 축소되었던 자원이 첫 요청을 받아 컨테이너를 시작하고, 거대한 LLM 모델(수십 GB)을 GPU 메모리에 로드하는 데 수십 초에서 수 분까지 걸릴 수 있음.
*   **완화 전략**:
    *   **프로비저닝된 동시성 (Provisioned Concurrency)**: 일정 수의 인스턴스를 항상 활성 상태(Warm)로 유지하여 콜드 스타트를 피함. (서버리스의 비용 장점 희석)
    *   **모델 최적화**: 모델 경량화(Quantization, Pruning)나 FlashAttention 같은 기술로 로딩 및 추론 속도 개선.
    *   **특화 플랫폼 활용**: Modal, Banana.dev, RunPod 등 콜드 스타트 최적화에 특화된 서버리스 GPU 플랫폼 사용.

### 2.3 GPU 자원 관리 및 최적화

GPU는 비싼 자원이므로 효율적으로 사용하는 것이 매우 중요함.

*   **vLLM & PagedAttention**: 여러 요청을 하나의 배치(Batch)로 묶어 처리하고, PagedAttention 기술로 KV 캐시 메모리를 효율적으로 관리하여 동일한 GPU에서 더 많은 동시 요청을 처리할 수 있게 함 (처리량 증대).
*   **모델 양자화 (Quantization)**: 모델의 가중치를 32비트 부동소수점에서 8비트나 4비트 정수로 변환하여, 모델 크기를 줄이고 추론 속도를 높임. 약간의 성능 저하가 있을 수 있음.
*   **스팟 인스턴스 (Spot Instances)**: 클라우드의 유휴 GPU 자원을 매우 저렴하게 사용할 수 있지만, 언제든지 중단될 수 있음. 긴급하지 않은 학습이나 배치 작업에 적합.

---

## 3. 예시 (Example)

### 사용 사례별 배포 아키텍처 선택

*   **사례 1: 실시간 고객 지원 챗봇**
    *   **요구사항**: 24/7 안정적이고 빠른 응답.
    *   **선택**: **Kubernetes 클러스터**에 **Docker 컨테이너**로 에이전트를 배포. 트래픽에 따라 자동으로 확장(HPA)되도록 설정하고, **vLLM**이 적용된 **전용 A100 GPU** 인스턴스를 사용하여 높은 처리량을 보장.

*   **사례 2: 매일 밤 주식 시장 데이터를 분석하고 리포트를 생성하는 에이전트**
    *   **요구사항**: 정해진 시간에만 작동. 비용 최소화.
    *   **선택**: **Google Cloud Functions** 또는 **AWS Lambda** 같은 **서버리스** 함수로 에이전트를 구현. 스케줄러(Cron)를 통해 매일 밤 정해진 시간에만 함수를 트리거. GPU가 필요한 경우, 서버리스 GPU 플랫폼을 연동하여 작업 시간 동안만 비용을 지불.

*   **사례 3: 코드 리뷰 멀티-에이전트 시스템**
    *   **요구사항**: 여러 에이전트(코드 분석, 테스트 케이스 생성, 리뷰 작성) 간의 복잡한 상호작용.
    *   **선택**: **LangGraph**를 사용하여 에이전트들의 워크플로우를 정의하고, 이를 **Kubernetes** 클러스터에 배포. 각 에이전트는 별도의 컨테이너로 실행되며, 공유 상태 저장을 위해 Redis와 같은 인메모리 DB를 사용.

---

## 4. 예상 면접 질문 (Potential Interview Questions)

*   **Q. 실시간 응답이 중요한 챗봇 에이전트를 배포할 때, 서버리스 아키텍처가 일반적으로 추천되지 않는 이유는 무엇인가요?**
    *   **A.** 콜드 스타트 문제 때문입니다. 서버리스는 유휴 상태일 때 자원을 0으로 축소하여 비용을 절감하는데, 첫 요청이 오면 컨테이너를 시작하고 수십 GB의 LLM 모델을 GPU에 로드하는 데 수십 초가 걸릴 수 있습니다. 이는 실시간 대화 경험에 치명적인 지연을 유발하므로, 항상 준비 상태를 유지하는 컨테이너 기반 배포가 더 적합합니다.

*   **Q. vLLM과 PagedAttention 기술이 에이전트 서빙의 처리량(Throughput)을 어떻게 향상시키나요?**
    *   **A.** LLM 추론 시 가장 큰 병목 중 하나는 GPU 메모리, 특히 이전 토큰들의 Key/Value를 저장하는 KV 캐시입니다. PagedAttention은 이 KV 캐시를 운영체제의 페이징처럼 관리하여 메모리 파편화를 줄이고, 여러 요청 간에 메모리를 효율적으로 공유할 수 있게 해줍니다. vLLM은 이를 활용하여 더 많은 요청을 하나의 배치로 묶어 동시에 처리할 수 있으므로, 결과적으로 단위 시간당 처리량(Throughput)이 크게 향상됩니다.

*   **Q. 에이전트 배포 시 비용을 최적화할 수 있는 방안 3가지를 제시해주세요.**
    *   **A.** 첫째, 워크로드에 맞는 아키텍처를 선택하는 것입니다. 간헐적인 작업에는 서버리스를 사용하여 유휴 비용을 없앨 수 있습니다. 둘째, 모델 양자화(Quantization)를 통해 모델 크기를 줄여 더 저렴한 GPU에서도 실행하거나, 추론 속도를 높여 비용을 절감할 수 있습니다. 셋째, 긴급하지 않은 학습이나 배치 분석 작업에는 언제든 중단될 수 있지만 가격이 매우 저렴한 스팟 인스턴스를 활용하는 것입니다.

---

## 5. 더 읽어보기 (Further Reading)

*   [vLLM Project Documentation](https://docs.vllm.ai/en/latest/)
*   [Serverless GPUs: The next frontier of computing (Modal Blog)](https://modal.com/blog/serverless-gpus)
*   [Google Agent Document (Deployment Considerations)](https://www.kaggle.com/whitepaper-agent-companion)